{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS in my system :  linux\n",
      "\n",
      "Skriptas papildo vietinę ZIVE talpyklą duomenimis, esančiais buferyje\n",
      "Duomenų buferio aplankas: /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/buffer\n",
      "Vietinės talpyklos aplankas: /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records\n",
      "\n",
      "Sukuriamas naujas Zive EKG įrašų rinkinys aplanke: /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records\n",
      "\n",
      "\n",
      "Rasta naujų įrašų buferyje: /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/buffer: 4\n",
      "\n",
      "Naujų įrašų sąrašas\n",
      "\n",
      "     file_name                    userId               recordingId    N   S    V  U  incl  flag         recorded_at\n",
      "0  1630844.567  6144c5b1bd0cc5a681275363  6145aeb6bd0cc5e8392757d1  903   0    0  0     0     0 2021-09-05 12:22:47\n",
      "1  1631076.286  6144c4fbbd0cc552e427535f  6144cd8bbd0cc50b5e2753b6  541   0  194  0     0     0 2021-09-08 04:44:46\n",
      "3  1631134.867  613b1d673d08d4d1f3cdc8f8  613b22d43d08d407b7cdc9df  656  12    0  0     0     0 2021-09-08 21:01:07\n",
      "2  1636452.484  6190d4b23cd1d29db2303ce9  6190db883cd1d2635c3041c9  600  11    2  0     0     0 2021-11-09 10:08:04\n",
      "\n",
      "Atnaujintas failų sąrašas įrašytas:  /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records/list.json\n",
      "Atnaujintas failų sąrašas įrašytas į:  /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records/list.csv\n",
      "\n",
      "Zive EKG įrašų rinkinys po papildymo: /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records  įrašų kiekis: 4\n",
      "\n",
      "     file_name                    userId               recordingId    N   S    V  U  incl  flag         recorded_at\n",
      "0  1630844.567  6144c5b1bd0cc5a681275363  6145aeb6bd0cc5e8392757d1  903   0    0  0     0     0 2021-09-05 12:22:47\n",
      "1  1631076.286  6144c4fbbd0cc552e427535f  6144cd8bbd0cc50b5e2753b6  541   0  194  0     0     0 2021-09-08 04:44:46\n",
      "2  1631134.867  613b1d673d08d4d1f3cdc8f8  613b22d43d08d407b7cdc9df  656  12    0  0     0     0 2021-09-08 21:01:07\n",
      "3  1636452.484  6190d4b23cd1d29db2303ce9  6190db883cd1d2635c3041c9  600  11    2  0     0     0 2021-11-09 10:08:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1988/1326942438.py:127: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list_new = df_list_new.append(df_row, ignore_index=True)\n",
      "/tmp/ipykernel_1988/1326942438.py:127: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list_new = df_list_new.append(df_row, ignore_index=True)\n",
      "/tmp/ipykernel_1988/1326942438.py:127: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list_new = df_list_new.append(df_row, ignore_index=True)\n",
      "/tmp/ipykernel_1988/1326942438.py:127: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_list_new = df_list_new.append(df_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "#\n",
    "#  Failų vardai dabar rodo timestamp\n",
    "# Skriptas suranda visus json failus užduotame buferiniame folderyje db_path_suppl, patikrina, ar jų nėra \n",
    "# vietinėje talpykloje (folderis db_path) ir tada juos perrašo į db_path, papildydamas sarašus list.json, list.xlsx.\n",
    "# Sarašas saugomas json formatu, gali būti rankomis redaguojamas, jame yra požymis 'incl', \n",
    "# kuris lygus 1, jei failas įtraukiamas į analizę. Taip pat yra papildomi flagai F1, F2, F3 kitiems tikslams.\n",
    "\n",
    "import os, shutil, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas\n",
    "db_folder = 'ZIVE_BUFFER_2022_03_24'\n",
    "\n",
    "# Duomenų buferio aplankas\n",
    "db_folder_suppl = 'buffer'\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "print('\\nSkriptas papildo vietinę ZIVE talpyklą duomenimis, esančiais buferyje')\n",
    "\n",
    "# Nuorodos į aplankus su EKG duomenų rinkiniu ir duomenų buferiu\n",
    "db_path_suppl = Path(Duomenu_aplankas, db_folder, db_folder_suppl) # Buferis, iš kurio kaupiama\n",
    "db_path = Path(Duomenu_aplankas, db_folder, 'records') # vietinė talpykla su jau esamais duomenimis arba tuščia\n",
    "\n",
    "print( 'Duomenų buferio aplankas:', db_path_suppl)\n",
    "print( 'Vietinės talpyklos aplankas:', db_path)\n",
    "\n",
    "# - paruošiame dataframe įrašų sužymėjimui\n",
    "df_list_new = pd.DataFrame({\n",
    "                        'file_name': pd.Series(dtype='str'),\n",
    "                        'userId': pd.Series(dtype='str'),\n",
    "                        'recordingId': pd.Series(dtype='str'),\n",
    "                        'N': pd.Series(dtype='int'),\n",
    "                        'S': pd.Series(dtype='int'),\n",
    "                        'V': pd.Series(dtype='int'),\n",
    "                        'U': pd.Series(dtype='int'),\n",
    "                        'incl': pd.Series(dtype='int'), \n",
    "                        'flag': pd.Series(dtype='int'),\n",
    "                        'recorded_at':pd.Series(dtype='datetime64[ns]')})\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 16)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Nuskaitomas, jei yra, jau egzistuojantis sęrašas\n",
    "list_exists = False\n",
    "filepath = Path(db_path,'list.json') \n",
    "if os.path.isfile(filepath):\n",
    "    list_exists = True\n",
    "    df_list = pd.read_json(filepath, orient = 'table')\n",
    "    df_len = len(df_list)\n",
    "    print(f'\\nTurimas Zive EKG įrašų rinkinys: {db_path}  įrašų kiekis: {df_len}\\n')\n",
    "    # print(df_list)\n",
    "    nr = df_len\n",
    "else:\n",
    "    # Jei nėra, bus kuriamas naujas sąrašas\n",
    "    print(f'\\nSukuriamas naujas Zive EKG įrašų rinkinys aplanke: {db_path}\\n')\n",
    "    nr = 0\n",
    "\n",
    "\n",
    "# - surenkame aplankų ir failų sąrašą\n",
    "# https://careerkarma.com/blog/python-list-files-in-directory/\n",
    "\n",
    "# Walking a directory tree and printing the names of the directories and files\n",
    "file_list = []\n",
    "filepaths =[]\n",
    "new_files = 0\n",
    "\n",
    "# Surandame buferyje visus failus, kurie neturi extension json\n",
    "for dirpath, dirnames, files in os.walk(db_path_suppl):\n",
    "    # https://linuxhint.com/python-os-walk-example/\n",
    "    # print(dirpath, dirnames, files)\n",
    "    for file_name in files:\n",
    "        root, extension = os.path.splitext(file_name)\n",
    "        if (extension != '.json'):\n",
    "            # print(file_name)\n",
    "            filepath = Path(dirpath,file_name)\n",
    "            filepaths.append(str(filepath))\n",
    "\n",
    "# Einame per visus buferyje esančius įrašus, įtraukiame į vietinę talpyklą, jei jų ten nėra\n",
    "for filepath in filepaths:\n",
    "    filename = os.path.basename(filepath)\n",
    "    timestamp = int(filename.replace('.', ''))\n",
    "    # recorded_at = datetime.fromtimestamp(timestamp, tz=timezone.utc)\n",
    "          \n",
    "    filepath = Path(filepath + '.json')\n",
    "    with open(filepath,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        data = json.loads(f.read())\n",
    "    dict_annot = data['rpeakAnnotationCounts']\n",
    "    df_row = {'file_name':filename, 'userId':data['userId'], 'recordingId':data['recordingId'],\n",
    "    'N':dict_annot.get('N',0), 'S':dict_annot.get('S',0), 'V':dict_annot.get('V',0),\n",
    "    'U':dict_annot.get('U',0),'incl':0,'flag':0,'recorded_at':timestamp}\n",
    "    \n",
    "    flag_include = False\n",
    "    if (list_exists == False): # sąrašas neegzistuoja, įtraukiame visus į naują sąrašą\n",
    "        flag_include = True\n",
    "    else:   # Sąrašas egzistuoja, įtraukiame į naują sąrašą tik tuos, kurių nėra sename\n",
    "        if filename not in  df_list['file_name'].values:    \n",
    "            flag_include = True\n",
    "    \n",
    "    if flag_include == True:    \n",
    "        # df_list = df_list.append(df_row, ignore_index=True)\n",
    "        df_list_new = df_list_new.append(df_row, ignore_index=True)\n",
    "        nr +=1\n",
    "        new_files +=1\n",
    "        src = Path(db_path_suppl, filename)\n",
    "        dst = Path(db_path, filename)\n",
    "\n",
    "        # kopijuojame duomenis ir .json failus, išsaugodami metaduomenis su copy2\n",
    "        shutil.copy2(src, dst)\n",
    "        src = Path(db_path_suppl, filename + '.json')\n",
    "        dst = Path(db_path, filename + '.json')\n",
    "        shutil.copy2(src, dst)\n",
    "        \n",
    "print(f'\\nRasta naujų įrašų buferyje: {db_path_suppl}: {new_files}')\n",
    "if (new_files != 0):\n",
    "    df_list_new['recorded_at'] =  pd.to_datetime(df_list_new['recorded_at'], unit='s')\n",
    "    df_list_new = df_list_new.convert_dtypes() # koreguoju dtypes į geriau atitinkančius turinį\n",
    "    df_list_new.sort_values(by='recorded_at', ascending=True, inplace = True)\n",
    "    df_list_new.reset_index()\n",
    "    file_path = Path(db_path,'list_new.json')       \n",
    "    df_list_new.to_json(file_path, orient = 'table', index=False)\n",
    "    # df_list_new = pd.read_json(file_path, orient = 'table')\n",
    "    # Bandžiau įrašyti ir nuskaityti įrašų papildymo sąrašą prieš spausdindamas, \n",
    "    # nes nuskaičius json integer timestamp pavirsta datetime. Tačiau\n",
    "    # to nebereikia, nes paverčiau su pd.to_datime\n",
    "    print(f'\\nNaujų įrašų sąrašas\\n')\n",
    "    print(df_list_new)\n",
    "\n",
    "if (list_exists == True): # jei sąrašas jau egzistuoja, siujungiame su nauju\n",
    "    df_list =  pd.concat([df_list,df_list_new], ignore_index=True)\n",
    "else:\n",
    "    df_list = df_list_new\n",
    "    list_exists= True\n",
    "\n",
    "df_list.sort_values(by='recorded_at', ascending=True, inplace = True)\n",
    "# print(df_list.dtypes)\n",
    "\n",
    "if new_files != 0:\n",
    "    file_path = Path(db_path,'list.json')       \n",
    "    df_list.to_json(file_path, orient = 'table', index=False)\n",
    "    print(f'\\nAtnaujintas failų sąrašas įrašytas:  {file_path}')\n",
    "\n",
    "    file_path = Path(db_path,'list.csv') \n",
    "    df_list.to_csv(file_path)\n",
    "    print(f'Atnaujintas failų sąrašas įrašytas į:  {file_path}')    \n",
    "\n",
    "if (list_exists):\n",
    "    filepath = Path(db_path,'list.json') \n",
    "    df_list = pd.read_json(filepath, orient = 'table')\n",
    "    df_len = len(df_list)\n",
    "    print(f'\\nZive EKG įrašų rinkinys po papildymo: {db_path}  įrašų kiekis: {df_len}\\n')\n",
    "    print(df_list)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
