{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\n",
      "Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio\n",
      "OS in my system :  linux\n",
      "\n",
      "Bendras duomenų aplankas:  /home/kesju/DI/Data/MIT&ZIVE\n",
      "Zive duomenų aplankas:  ZIVE_BUFFER_2022_03_24\n",
      "Aplankas su originaliais EKG įrašais ir anotacijomis (.json)  /home/kesju/DI/Data/MIT&ZIVE/ZIVE_BUFFER_2022_03_24/records_selected\n",
      "Pūpsnių atributų failas: all_beats_attr_z.csv\n",
      "Diskretizavimo dažnis:  200\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "Modelio ir scaler parametrai nuskaitomas iš aplanko:  model_cnn_fda_vu_v1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos paimamos iš\n",
    "#  aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys iš VU_DUOM 50 pacientų\n",
    "# įrašų saugyklos, jame yra ir failas all_beats_attr\n",
    " \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from icecream import ic\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from zive_util_vu import cm2df, show_confusion_matrix \n",
    "from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "from zive_util_vu import runtime, split_SubjCode\n",
    "from zive_util_vu import get_freq_unique_values, zive_read_file_1ch\n",
    "\n",
    "from zive_util_vu import get_beat_attributes\n",
    "from zive_util_vu import get_userId, get_rec_Id \n",
    "from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "from zive_cnn_fda_vu_v1 import predict_cnn_fda_vu_v1, classify_cnn_fda_vu_v1, get_pred_symbols\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\")\n",
    "print('Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio')\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'ZIVE_BUFFER_2022_03_24'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3, 'F':3}  \n",
    "\n",
    "# Diskretizavimo dažnis:\n",
    "fs = 200\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_selected')\n",
    "\n",
    "# Nuoroda į modelio aplanką\n",
    "# model_dir = Path(Duomenu_aplankas, 'DNN', 'best_models', 'all_ft')\n",
    "model_dir = 'model_cnn_fda_vu_v1'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su originaliais EKG įrašais ir anotacijomis (.json) \", rec_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "print(\"Modelio ir scaler parametrai nuskaitomas iš aplanko: \", model_dir)\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atliekama pūpsnių pacientų įrašuose klasifikacija\n",
      "Klasifikuojamų įrašų sąrašas: ['1000.1631134.867']\n",
      "['N', 'S', 'V']\n",
      "\n",
      "1000.1631134.867                1631134.867\n",
      "signal_length=  127999 (127999,)\n",
      "test_labels:  [0 1] [656  12] 668\n",
      "pred_labels:  [0 1 2 3] [623  40   2   3] 668\n",
      "i_sample: 205 pred_label: 3\n",
      "i_sample: 4498 pred_label: 1\n",
      "i_sample: 4653 pred_label: 1\n",
      "i_sample: 7407 pred_label: 1\n",
      "i_sample: 7641 pred_label: 1\n",
      "i_sample: 11339 pred_label: 2\n",
      "i_sample: 18333 pred_label: 3\n",
      "i_sample: 18947 pred_label: 1\n",
      "i_sample: 19100 pred_label: 1\n",
      "i_sample: 20504 pred_label: 1\n",
      "i_sample: 20648 pred_label: 1\n",
      "i_sample: 21872 pred_label: 1\n",
      "i_sample: 22000 pred_label: 1\n",
      "i_sample: 22127 pred_label: 1\n",
      "i_sample: 22251 pred_label: 1\n",
      "i_sample: 22625 pred_label: 1\n",
      "i_sample: 22765 pred_label: 1\n",
      "i_sample: 23459 pred_label: 1\n",
      "i_sample: 30407 pred_label: 1\n",
      "i_sample: 30555 pred_label: 1\n",
      "i_sample: 34221 pred_label: 1\n",
      "i_sample: 41433 pred_label: 1\n",
      "i_sample: 62102 pred_label: 1\n",
      "i_sample: 66679 pred_label: 1\n",
      "i_sample: 67354 pred_label: 1\n",
      "i_sample: 67655 pred_label: 1\n",
      "i_sample: 70303 pred_label: 1\n",
      "i_sample: 72400 pred_label: 1\n",
      "i_sample: 74999 pred_label: 1\n",
      "i_sample: 75415 pred_label: 1\n",
      "i_sample: 77278 pred_label: 1\n",
      "i_sample: 77623 pred_label: 1\n",
      "i_sample: 78312 pred_label: 1\n",
      "i_sample: 86767 pred_label: 1\n",
      "i_sample: 105978 pred_label: 1\n",
      "i_sample: 106129 pred_label: 1\n",
      "i_sample: 106278 pred_label: 1\n",
      "i_sample: 106433 pred_label: 1\n",
      "i_sample: 106923 pred_label: 1\n",
      "i_sample: 107623 pred_label: 2\n",
      "i_sample: 119525 pred_label: 1\n",
      "i_sample: 120950 pred_label: 1\n",
      "i_sample: 123075 pred_label: 1\n",
      "i_sample: 123851 pred_label: 1\n",
      "i_sample: 127967 pred_label: 3\n",
      "N:  653 S: 12 V:  0  Nprec: 1.00 Nrec: 0.95 Nfsc: 0.98  Sprec: 0.25 Srec: 0.83 Sfsc: 0.38  Vprec: 0.00 Vrec: 0.00 Vfsc: 0.00\n",
      "\n",
      "\n",
      "Runtime: 00:00:22\n"
     ]
    }
   ],
   "source": [
    "# PASIRUOŠIMAS\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Naudojamų požymių sąrašas\n",
    "\n",
    "all_features = ['RRl', 'RRr', 'RRl/RRr',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "\n",
    "# Pacientų įrašų kodų sąrašas iš Zive saugyklos:\n",
    "SubjCodes = \\\n",
    " [10000, 10001, 10002, 10003, 10010, 10020, 10021, 10022, 10023, 10024, 10025, 10030, 10031, 10032, 10033, 10034, 10035,\n",
    " 10036, 10040, 10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060, 10061, 10062, 10063, 10064,\n",
    " 10065, 10066, 10067, 10068, 10069, 10070, 10071, 10072, 10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088,\n",
    " 10089, 10090, 10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109, 10110, 10120, 10121, 10122, 10123,\n",
    " 10124, 10125, 10126, 10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10140, 10141, 10142, 10143, 10144, 10145,\n",
    " 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162,\n",
    " 10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176, 10180, 10190, 10191,\n",
    " 10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200, 10201, 10210, 10211, 10212, 10213, 10214, 10215, 10216,\n",
    " 10217, 10220, 10221, 10230, 10231, 10232, 10233, 10234, 10235, 10236, 10237, 10238, 10239, 10240, 10241, 10250, 10260,\n",
    " 10270, 10280, 10281, 10290, 10291, 10300, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 10310, 10311,\n",
    " 10312, 10313, 10314, 10315, 10316, 10317, 10318, 10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326, 10327, 10328,\n",
    " 10329, 10330, 10331, 10332, 10333, 10334, 10335, 10340, 10341, 10342, 10343, 10344, 10345, 10346, 10347, 10348, 10349,\n",
    " 10350, 10351, 10352, 10353, 10354, 10355, 10356, 10360, 10361, 10362, 10370, 10371, 10380, 10381, 10382, 10390, 10400,\n",
    " 10401, 10410, 10411, 10420, 10421, 10422, 10423, 10424, 10425, 10426, 10427, 10428, 10429, 10430, 10431, 10432, 10433,\n",
    " 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10443, 10444, 10445, 10450, 10451, 10452, 10453, 10454,\n",
    " 10455, 10456, 10457, 10458, 10459, 10460, 10461, 10462, 10470, 10471, 10472, 10473, 10474, 10475, 10480, 10481, 10482,\n",
    " 10483, 10484, 10485, 10486, 10487, 10488, 10489, 10490, 10491, 10492, 10493, 10494, 10495, 10496, 10497, 10498, 10499]\n",
    "\n",
    "SubjCodes = [\"1000.1631134.867\", \"1001.1631076.286\", \"1002.1630844.567\", \"1003.1636452.484\"] #Testavimui\n",
    "# SubjCodes = [\"1000.1631134.867\"] #Testavimui\n",
    "\n",
    "# Išvedamas pacientų įrašų sąrašas\n",
    "print(\"\\nAtliekama pūpsnių pacientų įrašuose klasifikacija\")\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "print(class_names)\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(rec_dir, all_beats_attr_fname)\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'file_name': str,\n",
    "                                                             'sample': int, 'symbol': str, 'label': int})\n",
    "all_beat_indices = all_beats_attr.index\n",
    "\n",
    "index_start = 0\n",
    "validation_set_stats = pd.DataFrame(columns=['idx', 'test_label', 'pred_label', 'SubjCode'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Ciklas per pacientų įrašus\n",
    "for SubjCode in SubjCodes:\n",
    "    userNr, file_name = split_SubjCode(SubjCode)\n",
    "    # userId, recordingId =  get_rec_Id(rec_dir, userNr, recordingNr)\n",
    "    print(f\"\\n{SubjCode} {str(file_name):>26}\")\n",
    "\n",
    "    # Paciento EKG sekų formavimas ir klasifikacija  \n",
    "    file_path = Path(rec_dir, file_name) # Dirbame su originaliais duomenimis\n",
    "    sign_raw = zive_read_file_1ch(file_path)  \n",
    "    signal_length = sign_raw.shape[0]\n",
    "    print('signal_length= ', signal_length, sign_raw.shape)\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = zive_read_df_rpeaks(rec_dir, file_name)\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Nereikalingiems 'U' ir 'F' suteikiam klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "\n",
    "    # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "    userNr, file_name = split_SubjCode(SubjCode)\n",
    "    selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['file_name']==file_name)]\n",
    "    # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "    index_start = selected_ind[0]\n",
    "    # print('\\nSubjCode:',SubjCode, 'index_start:', index_start)   \n",
    "\n",
    "    pred_labels = predict_cnn_fda_vu_v1(signal, atr_sample, model_dir)\n",
    "    \n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \",unique, counts, total)\n",
    "\n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        classification.append({'sample':i_sample, 'annotation':pred_labels[i]})\n",
    "        if (pred_labels[i] != 0):\n",
    "            print(f\"i_sample: {i_sample} pred_label: {pred_labels[i]}\")  \n",
    "\n",
    "    #  Praleisdami indeksą, jei masyvuose test_labels ir pred_labels yra reikšmė == 3,\n",
    "    # suformuojame klasifikuotinų pūpsnių indeksų sąrašą\n",
    "    for idx in range(len(atr_sample)):\n",
    "        flag = (test_labels[idx] == 3) or (pred_labels[idx] == 3)\n",
    "        if (flag == False):\n",
    "            validation_set_stats = validation_set_stats.append({'idx':index_start+idx,\n",
    "            'test_label':test_labels[idx],'pred_label':pred_labels[idx], 'SubjCode': SubjCode}, ignore_index=True)\n",
    "\n",
    "    # SURANDAME TIKSLUMO PARAMETRUS\n",
    "    test_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['test_label']).astype('int') \n",
    "\n",
    "    # print(all_beats_attr.info())\n",
    "\n",
    "    pred_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['pred_label']).astype('int')\n",
    "    confusion = confusion_matrix(test_y, pred_y)\n",
    "    # print(confusion)\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_y, pred_y, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(sup[0]):>5} S:{(int(sup[1])):3} V:{int(sup[2]):3}\" \n",
    "    str2 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str3 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str4 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "    # print(len(validation_set_stats))\n",
    "    # print(len(test_y))\n",
    "    # print(len(pred_y))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('\\n')\n",
    "    runtime(end_time-start_time)\n",
    "\n",
    "validate_ind_lst = list(validation_set_stats['idx'])\n",
    "y_validate = np.array(validation_set_stats['test_label']).astype('int')\n",
    "y_predicted = np.array(validation_set_stats['pred_label']).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODELIO TIKSLUMO VERTINIMO iš VERTINIMO IMTIES REZULTATAI\n",
      "Modelis iš aplanko:  model_cnn_fda_vu_v1\n",
      "Klasės ['N', 'S', 'V']: [653  12] Suma: 665 \n",
      "\n",
      "APIBENDRINTI REZULTATAI\n",
      "\n",
      "Confusion Matrix\n",
      "     N   S  V\n",
      "N  623  30  0\n",
      "S    0  10  2\n",
      "V    0   0  0\n",
      "\n",
      "\n",
      "Zero values! Cannot calculate Normalized Confusion Matrix\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      1.000     0.954     0.976       653\n",
      "           S      0.250     0.833     0.385        12\n",
      "           V      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.952       665\n",
      "   macro avg      0.417     0.596     0.454       665\n",
      "weighted avg      0.986     0.952     0.966       665\n",
      "\n",
      "\n",
      "Apibendrinti_rezultatai įrašyti į:  model_cnn_fda_vu_v1/Apibendrinti_rezultatai.csv\n"
     ]
    }
   ],
   "source": [
    "# MODELIO TIKSLUMO VERTINIMO IŠ VERTINIMO IMTIES REZULTATAI\n",
    "\n",
    "print(\"\\nMODELIO TIKSLUMO VERTINIMO iš VERTINIMO IMTIES REZULTATAI\")\n",
    "print(\"Modelis iš aplanko: \", model_dir)\n",
    "\n",
    "cols, dist, tot = get_freq_unique_values(y_validate, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot} \")\n",
    "\n",
    "# APIBENDRINTI REZULTATAI\n",
    "\n",
    "print('\\nAPIBENDRINTI REZULTATAI\\n')\n",
    "\n",
    "# Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "confusion = confusion_matrix(y_validate, y_predicted)\n",
    "pd.set_option('display.precision',3)\n",
    "show_confusion_matrix(confusion, class_names)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"\\nClassification Report\\n\")\n",
    "# target_names = [key for (key, value) in selected_beats.items()]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(classification_report(y_validate, y_predicted, target_names=class_names, digits=3))\n",
    "report = classification_report(y_validate, y_predicted, target_names=class_names, output_dict=True)\n",
    "# output_dictbool, default=False, If True, return output as dict.\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# https://medium.com/@asmaiya/you-can-something-like-this-84d28e0fd31f\n",
    "\n",
    "# Įrašome į diską\n",
    "filepath = Path(model_dir, 'Apibendrinti_rezultatai.csv') \n",
    "df_report.to_csv(filepath)    \n",
    "print(f'\\nApibendrinti_rezultatai įrašyti į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultatų pasiskirstymas per pacientus\n",
      "Pacientų: 1\n",
      "\n",
      "userNr                   userId        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "  1000 613b1d673d08d4d1f3cdc8f8      653   12    0    1.00  0.95  0.98    0.25  0.83  0.38    0.00  0.00  0.00      4.8     32.6\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientus įrašytas į:  model_cnn_fda_vu_v1/Rezultatu_pasiskirstymas_per_pacientus.csv\n"
     ]
    }
   ],
   "source": [
    "# KLAIDŲ PASISKIRSTYMAS PER PACIENTUS IR JŲ ĮRAŠUS\n",
    "\n",
    "from zive_util_vu import zive_read_df_data, create_SubjCode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def collect_noise_locs(rec_dir, all_beats_attr):\n",
    "\n",
    "    noise_arr_tot = np.empty(0, dtype=int)\n",
    "    \n",
    "    grouped = all_beats_attr.groupby(['userNr', 'file_name'])\n",
    "    for key, group in grouped:\n",
    "        # print('\\n',key)\n",
    "        userNr = key[0]\n",
    "        file_name = key[1]\n",
    "\n",
    "        # SubjCode naudojamas atveju, kai duomenis yra formoje SubjCode.npy, SubjCode.json\n",
    "        # SubjCode = create_SubjCode(userNr, file_name)\n",
    "        # filepath = Path(rec_dir, SubjCode + '.json')  # Si eilute pritaikyta atvejui,\n",
    "        filepath = Path(rec_dir, file_name + '.json')\n",
    "        df_noises = zive_read_df_data(filepath, 'noises')\n",
    "\n",
    "        noise_arr = np.full(shape=len(group), fill_value=0,  dtype=int)\n",
    "        idx_noise = 0\n",
    "\n",
    "        for i, row_i in group.iterrows():\n",
    "            sample = row_i['sample']\n",
    "            for j, row_j in df_noises.iterrows():\n",
    "                if ((sample > row_j['startIndex']) & (sample < row_j['endIndex'])):\n",
    "                    noise_arr[idx_noise] = 1\n",
    "                    idx_noise += 1\n",
    "        noise_arr_tot = np.append(noise_arr_tot, noise_arr)\n",
    "    return noise_arr_tot\n",
    "\n",
    "def get_error(y_test, y_pred):\n",
    "    # Error in %\n",
    "    n_errors = 0\n",
    "    for idx in range(len(y_pred)):\n",
    "        if (y_test[idx] != y_pred[idx]):\n",
    "            n_errors += 1\n",
    "    Err = float(n_errors)/len(y_pred)*100.\n",
    "    Err = round(Err, 1)\n",
    "    return Err\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Sukuriame ir užpildome dataframe su sekų parametrais\n",
    "df_seq_errors = pd.DataFrame(columns= ['idx', 'userNr', 'file_name'])\n",
    "\n",
    "rows_list = []\n",
    "for idx in validate_ind_lst:\n",
    "# Surandame  userNr, file_name, symbol\n",
    "    userNr, file_name, label, symbol = get_beat_attributes(idx, all_beats_attr)\n",
    "    seq_attr = {'idx': idx, 'userNr':userNr, 'file_name':file_name}\n",
    "    rows_list.append(seq_attr)\n",
    "\n",
    "# print(rows_list[:10])\n",
    "\n",
    "# Čia įdedame informaciją, ar pūpsnys patenka į triukšmų zoną\n",
    "noise_arr = collect_noise_locs(rec_dir, all_beats_attr)\n",
    "noise_arr = noise_arr[validate_ind_lst]\n",
    "\n",
    "# print(noise_arr)\n",
    "# unique, counts = np.unique(noise_arr, return_counts=True)\n",
    "# print(unique, counts, noise_arr.shape[0])\n",
    "# print(counts.sum())\n",
    "\n",
    "df_seq_errors = pd.DataFrame(rows_list)\n",
    "\n",
    "df_seq_errors['labels'] = pd.Series(y_validate)\n",
    "df_seq_errors['preds'] = pd.Series(y_predicted)\n",
    "zeros_arr = np.zeros( y_predicted.shape[0], dtype=int)\n",
    "df_seq_errors['errors'] = pd.Series(zeros_arr)  \n",
    "df_seq_errors.loc[df_seq_errors['labels'] != df_seq_errors['preds'], 'errors'] = 1 \n",
    "df_seq_errors['noises'] = pd.Series(noise_arr)  \n",
    "\n",
    "# print(df_seq_errors.info())\n",
    "\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientus\n",
    "\n",
    "print(\"\\nRezultatų pasiskirstymas per pacientus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_user_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'), 'userId':pd.Series(dtype='str'), \n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "    'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Išgauname pacientų vidinę (eilės nr) numeraciją\n",
    "grouped  = df_seq_errors.groupby(['userNr'])\n",
    "userNrs = list(grouped.groups.keys())\n",
    "print(f'Pacientų: {len(userNrs)}\\n')\n",
    "\n",
    "for userNr in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[userNr]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[userNr]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "    \n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[userNr]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "    # Testavimui\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    dict_user_errors = {'userNr':int(userNr),'userId':str(userId), \n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err,  'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_user_errors)\n",
    "\n",
    "df_user_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "tit1 = f\"{'userNr':>6} {'userId':>24} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "for idx, row in  df_user_errors.iterrows():\n",
    "    str1 =f\"{int(row['userNr']):>6} {str(row['userId']):>6} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "    str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "    str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "    str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(model_dir, 'Rezultatu_pasiskirstymas_per_pacientus.csv') \n",
    "df_user_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per pacientus įrašytas į:  {filepath}')\n",
    "\n",
    "# print(df_user_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientų įrašus\n",
      "Pacientų įrašų: 1\n",
      "\n",
      "\n",
      "userNr: 1000 userId: 613b1d673d08d4d1f3cdc8f8\n",
      "userNr                 file_name        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "  1000                1631134.867      653   12    0    1.00  0.95  0.98    0.25  0.83  0.38    0.00  0.00  0.00      4.8     32.6\n",
      "\n",
      "Rezultatų pasiskirstymas per įrašus įrašytas į:  model_cnn_fda_vu_v1/Rezultatu_pasiskirstymas_per_irasus.csv\n"
     ]
    }
   ],
   "source": [
    "# Rezultatų pasiskirstymas per pacientus ir jų įrašus\n",
    "# Skaičiuojama visoms 3 klasėms Precision(tikslumas), Recall (atgaminimas), Fscore (F rodiklis)\n",
    "# \n",
    "# https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac\n",
    "# \n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 18)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientų įrašus\n",
    "print(\"\\n\\nRezultatų pasiskirstymas per pacientų įrašus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_rec_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'userId':pd.Series(dtype='str'), 'file_name':pd.Series(dtype='str'),\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Sugrupuojame eilutes pagal 'userId','recordingId', suskaičiuojame, kiek kiekviename įraše\n",
    "# iš viso yra klasifikuojamų sekų (labels) ir kiek padaryta klaidų (errors).\n",
    "# Grupavimo objektą paverčiame į normalų dataframe objektą\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','file_name'])\n",
    "print(f'Pacientų įrašų: {grouped.ngroups}')\n",
    "# print(f'{grouped.size()=}')\n",
    "\n",
    "for key in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    # print(f'\\nGroup: {key}\\n{df_seq_errors.loc[grouped.groups[key]]}')\n",
    "    userNr = key[0]\n",
    "    file_name = key[1]\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[key]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[key]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "\n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[key]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "\n",
    "    # *********************** Testavimui *******************************************************************\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "    # *********************************************************************************************************\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    \n",
    "    dict_rec_errors = {'userNr':int(userNr), 'userId':userId, 'file_name':file_name,\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_rec_errors)\n",
    "\n",
    "df_rec_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "grouped  = df_rec_errors.groupby('userNr')\n",
    "for userNr, group in grouped:\n",
    "    # print(group.dtypes)\n",
    "    print(\"\\n\")\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "    print(f\"{'userNr:'} {userNr} {'userId:'} {userId}\" )\n",
    "    tit1 = f\"{'userNr':>6} {'file_name':>25} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "    tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "    tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "    tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "    print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        str1 =f\"{int(row['userNr']):>6} {str(row['file_name']):>26} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "        str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "        str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "        str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "        print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(model_dir, 'Rezultatu_pasiskirstymas_per_irasus.csv') \n",
    "df_rec_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per įrašus įrašytas į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'split_seq_file_name' from 'zive_util_vu' (/home/kesju/DI/VSC/ZIVEIO/VU_APRIL/zive_util_vu.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5268/1212369014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sukuriami užduoto ilgio sekų vaizdai ir įrašomi į disko atitinkamus klasėms subfolderius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mzive_util_vu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_rec_Id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_seq_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_symbol_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_SubjCode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_SubjCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzive_util_vu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_show_seq_ext_zive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_seq_start_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_rec_attrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'split_seq_file_name' from 'zive_util_vu' (/home/kesju/DI/VSC/ZIVEIO/VU_APRIL/zive_util_vu.py)"
     ]
    }
   ],
   "source": [
    "# Sukuriami užduoto ilgio sekų vaizdai ir įrašomi į disko atitinkamus klasėms subfolderius\n",
    "\n",
    "from zive_util_vu import get_rec_Id, split_seq_file_name, get_symbol_list, get_SubjCode, create_SubjCode\n",
    "from zive_util_vu import read_show_seq_ext_zive, read_seq, get_seq_start_end, read_rec_attrib\n",
    "\n",
    "\n",
    "def read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, win_ls, win_rs, win_ls_ext, win_rs_ext):\n",
    "# Išpjauna užduoto ilgio seką iš mit2zive įrašo ir sukuria jos vaizdą su anotacijomis\n",
    "\n",
    "# rec_dir - paciento EKG įrašų aplankas\n",
    "# recordingId - paciento EKG įrašo Id - int\n",
    "# i_sample - R dantelio, kurio atžvilgiu formuojama seka, indeksas viso EKG įrašo reikšmių masyve - int\n",
    "# win_ls - klasifikuojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs - klasifikuojamo EKG segmento plotis nuo R pūpsnio (iš dešinės)\n",
    "# win_ls_ext - vaizduojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs_ext - vaizduojamo EKG segmento plotis už R pūpsnio (iš dešinės) \n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    sequence, sample, label = read_seq(rec_dir, all_beats_attr, idx, win_ls_ext, win_rs_ext)\n",
    "    if (sample == None):\n",
    "        print(\"klaida!\")\n",
    "        return None\n",
    "    \n",
    "    seq_start = sample - win_ls_ext\n",
    "    seq_end = sample + win_ls_ext\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    atr_sample, atr_symbol = read_rec_attrib(rec_dir, SubjCode)\n",
    "\n",
    "    # # suformuojame anotacijų žymes\n",
    "    beat_symbols,beat_locs = get_symbol_list(atr_symbol,atr_sample, seq_start, seq_end)\n",
    "\n",
    "    # deltax ir deltay simbolių pozicijų koregavimui\n",
    "    min = np.amin(sequence)\n",
    "    max = np.amax(sequence)\n",
    "    deltay = (max - min)/20\n",
    "    deltax = len(sequence)/100\n",
    "\n",
    "    # suformuojame vaizdą\n",
    "    x = np.arange(0, len(sequence), 1)\n",
    "    ax.plot(x, sequence, color=\"#6c3376\", linewidth=2)\n",
    "    left_mark = win_ls_ext - win_ls\n",
    "    right_mark = win_ls_ext + win_rs\n",
    "    ax.axvline(x = left_mark, color = 'b', linestyle = 'dotted')\n",
    "    ax.axvline(x = right_mark, color = 'b', linestyle = 'dotted')\n",
    "    for i in range(len(beat_locs)):\n",
    "        ax.annotate(beat_symbols[i],(beat_locs[i]-deltax,sequence[beat_locs[i]]+deltay))\n",
    "    ax.set_ylim([min, max+2*deltay])\n",
    "    \n",
    "    return(ax)\n",
    "\n",
    "def get_seq_attributes(all_beats_attr, idx):\n",
    "    # 'userNr', 'recordingNr', 'sample', 'symbol', 'label', 'RRl', 'RRr'\n",
    "    row = all_beats_attr.loc[idx]\n",
    "    return list(row) \n",
    "\n",
    "# ////////////////////// Užduodami parametrai /////////////////////\n",
    "\n",
    "# Bendras aplankas vaizdams\n",
    "images_folder = 'CNN'\n",
    "\n",
    "# užduodame, kiek reikšmių vaizduosime prieš R dantelį ir po\n",
    "wl_side_ext = 360\n",
    "wr_side_ext = 360\n",
    "\n",
    "# Užduodame, kiek sekų vaizdų iš kiekvienos klasės įrašysime į diską\n",
    "img_max = 10\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "print(\"\\nBendras aplankas vaizdams:\", images_folder)\n",
    "print(\"\\nMax sekų vaizdų skaičius iš kiekvienos klasės:\", img_max)\n",
    "\n",
    "if (img_max == 0):\n",
    "    sys.exit()\n",
    "\n",
    "# sukuriame bendrą aplanką vaizdams\n",
    "images_dir = Path(sets_path, 'saved_images', images_folder)\n",
    "create_dir(images_dir)\n",
    "\n",
    "# klasių simbolinių vardų sąrašas\n",
    "class_names = selected_beats.keys()\n",
    "\n",
    "# sukuriame aplankus sekų vaizdams klasėse\n",
    "create_subdir(images_dir, class_names)\n",
    "\n",
    "# Sukuriame vaizdų klasėse skaitiklį\n",
    "n_classes = len(class_names)\n",
    "skait = np.zeros((n_classes, n_classes), dtype=int)\n",
    "# print(skait)\n",
    "\n",
    "# sukuriame selected_beats reversiją\n",
    "rev_dict = get_rev_dictionary(selected_beats)\n",
    "\n",
    "y_test = df_seq_errors['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors['preds'].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "# *********************** derinimui *******************************\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nAccuracy: {acc:.2f}\\n\")\n",
    "# cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "# show_confusion_matrix(cnf_matrix, class_names)\n",
    "# ******************************************************************\n",
    "\n",
    "# Ciklas per sekas\n",
    "\n",
    "icycle = 0 \n",
    "leng = len(y_pred)\n",
    "\n",
    "for idx in validate_ind_lst:\n",
    "    if (icycle >= leng):\n",
    "        continue\n",
    "\n",
    "# *********************** derinimui ******************************   \n",
    "    # row = get_seq_attributes(all_beats_attr, idx)\n",
    "    # print(idx, row)\n",
    "# ****************************************************************\n",
    "\n",
    "    # anotuotos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    label = y_test[icycle]\n",
    "    label_symb = rev_dict[label]\n",
    "\n",
    "    # print(f\" {idx} label {label} symb {label_symb}\")\n",
    "\n",
    "    # klasifikatoriaus priskirtos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    pred = y_pred[icycle]\n",
    "    pred_symb = rev_dict[pred]\n",
    "    icycle +=1\n",
    "\n",
    "    # print(f\" {idx} pred {pred} symb {pred_symb}\")\n",
    "\n",
    "    # patikriname, ar neviršytas skaitiklis, jei viršytas, peršokame\n",
    "    if (skait[label,pred] >= img_max):\n",
    "        continue\n",
    "    else:\n",
    "        skait[label,pred] += 1\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "    seq_name = str(SubjCode) + '_' + str(idx)\n",
    "    \n",
    "     # 'Išpjauname' užduoto ilgio sekas ir sukuriame jų vaizdus\n",
    "    fig = plt.figure(facecolor=(1, 1, 1), figsize=(18,3))\n",
    "    # print(\"rec_dir =\", rec_dir)\n",
    "    ax = read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, wl_side, wr_side, wl_side_ext, wr_side_ext) \n",
    "    if (ax == None):\n",
    "        print(f'Sekai {idx} negali suformuoti išplėstinio vaizdo')\n",
    "        plt.close()\n",
    "        continue\n",
    "\n",
    "    # suformuosime koreguotą failo vardą\n",
    "    file_name = seq_name + '_' + pred_symb + \".png\" \n",
    "\n",
    "    # suformuosime kelią į atitinkamą sub-aplanką\n",
    "    image_subdir = Path(images_dir, label_symb)\n",
    "    if (os.path.exists(image_subdir) == False):\n",
    "        print('Klaida! ', image_subdir,' neegzistuoja')\n",
    "    file_path = Path(image_subdir, file_name)\n",
    "    # print('file_name: ',file_name, 'file_path: ', file_path)   \n",
    "\n",
    "    # Įrašome į atitinkamą anotacijai sub-aplanką \n",
    "    ax.set_title(file_name)\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches = 0.2)\n",
    "    plt.close()\n",
    "\n",
    "    # if (icycle >= len(y_pred)):\n",
    "        # break\n",
    "\n",
    "# ciklo per validate_ind_lst pabaiga\n",
    "\n",
    "print(\"\\n\")\n",
    "df = cm2df(skait, class_names)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nPabaiga.........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zive įrašas:\n",
      "userNr: 1000  file_name: 1631134.867\n",
      "\n",
      "Anotacijų pasiskirstymas įraše:\n",
      "Klasės ['N', 'S', 'V']: [653  12] Suma: 665\n",
      " \n",
      "Accuracy: 0.95 Error(%): 4.8\n",
      " \n",
      "Confusion Matrix\n",
      "     N   S  V\n",
      "N  623  30  0\n",
      "S    0  10  2\n",
      "V    0   0  0\n",
      "\n",
      "\n",
      "Zero values! Cannot calculate Normalized Confusion Matrix\n",
      "\n",
      "\n",
      "       N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     653   12    0    1.00  0.95  0.98    0.25  0.83  0.38    0.00  0.00  0.00      4.8     32.6\n"
     ]
    }
   ],
   "source": [
    "# Klasifikavimo rodiklių skaičiavimas vienam nurodytam įrašui\n",
    "\n",
    "\n",
    "userNr, file_name = 1000, '1631134.867'\n",
    "\n",
    "# userId, recordingId =  get_rec_Id(rec_dir, userNr, file_name)\n",
    "print(\"\\nZive įrašas:\")\n",
    "print(f\"userNr: {userNr}  file_name: {file_name}\")\n",
    "# print(f\"\\nuserId: {userId}  recordingId: {recordingId}\")\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','file_name'])\n",
    "y_test = df_seq_errors.loc[grouped.groups[(userNr,file_name)]]['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors.loc[grouped.groups[(userNr,file_name)]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "print(\"\\nAnotacijų pasiskirstymas įraše:\")\n",
    "cols, dist, tot = get_freq_unique_values(y_test, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot}\\n \")\n",
    "\n",
    "noise_arr = df_seq_errors.loc[grouped.groups[(userNr,file_name)]]['noises'].to_numpy(dtype=int)\n",
    "Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "Err = get_error(y_test, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f} Error(%): {Err}\\n \")\n",
    "cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "show_confusion_matrix(cnf_matrix, class_names)\n",
    "# https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "# *********************************************************************************************************\n",
    "\n",
    "prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "row = {\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "\n",
    "print(\"\\n\")\n",
    "tit1 = f\"{'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "str1 =f\"{int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "print(str1+str2+str3+str4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
