{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skriptas Zive klasifikavimo rezultatų failo analizei\n",
      "OS in my system :  win32\n",
      "\n",
      "Bendras duomenų aplankas:  D:\\DI\\Data\\MIT&ZIVE\\VU\n",
      "Zive duomenų aplankas:  DUOM_VU\n",
      "Aplankas su EKG įrašais:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\n",
      "Aplankas su resultatais:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\test_results\\test\n",
      "Pūpsnių atributų failas: all_beats_attr_z.csv\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variantas:  vieno įrašo detalus testavimas \n",
    "\n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris arba paimamas if failo info_create.json,\n",
    "# arba iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "from icecream import ic\n",
    "import neurokit2 as nk\n",
    "\n",
    "\n",
    "# from zive_util_vu import cm2df, show_confusion_matrix \n",
    "# from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "# from zive_util_vu import runtime, split_SubjCode\n",
    "# from zive_util_vu import get_freq_unique_values\n",
    "\n",
    "# from zive_util_vu import get_beat_attributes\n",
    "# from zive_util_vu import get_userId, read_rec, get_filename \n",
    "# from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "# from zive_cnn_fda_vu_v1 import predict_cnn_fda_vu_v1, zive_read_file_1ch\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "\n",
    "print(\"Skriptas Zive klasifikavimo rezultatų failo analizei\")\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3}  \n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "\n",
    "res_dir = Path(db_path, 'test_results', 'test')\n",
    "# res_dir = '/home/test_results/test'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su EKG įrašais: \", rec_dir)\n",
    "print(\"Aplankas su resultatais: \", res_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikuojamų įrašų sąrašas: [10083]\n",
      "\n",
      "Zive įrašas:\n",
      "SubjCode: 10083 userNr: 1008 file_name: 1630757.924\n",
      "userId: 613b1d0c3d08d413ffcdc8f6  recordingId: 613f58593d08d48596cdcce9\n",
      "\n",
      "Nuskaitome ir analizuojame EKG įrašo anotacijas\n",
      "test_symbols:  ['N' 'S' 'V'] [723  11   8] 742\n",
      "test_labels:  [0 1 2] [723  11   8] 742\n",
      "D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_selected\\1630757.924\n",
      "\n",
      "Nuskaitome ir analizuojame klasifikacijos rezultatus\n",
      "file_path: D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\test_results\\test\\1630757.924\n",
      "pred_symbols:  ['N' 'S' 'U' 'V'] [678  29  16  19] 742\n",
      "pred_labels:  [0 1 2 3] [678  29  19  16] 742\n",
      "i,     json          docker\n",
      "\n",
      "Sutampa rpeaks: 735 iš 742\n",
      "\n",
      "Unique values in json that are not in res_docker:\n",
      "[ 10997  40592  53558  62393 103728 111407 125953]\n",
      "\n",
      "Unique values in res_docker that are not in res_json:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def read_rec(rec_dir, SubjCode):\n",
    "    file_path = Path(rec_dir, str(SubjCode) + '.npy')\n",
    "    signal = np.load(file_path, mmap_mode='r')\n",
    "    # print(f\"SubjCode: {SubjCode}  signal.shape: {signal.shape}\")\n",
    "    return signal\n",
    "\n",
    "def zive_read_file_1ch(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    a = np.fromfile(f, dtype=np.dtype('>i4'))\n",
    "    ADCmax=0x800000\n",
    "    Vref=2.5\n",
    "    b = (a - ADCmax/2)*2*Vref/ADCmax/3.5*1000\n",
    "    ecg_signal = b - np.mean(b)\n",
    "    return ecg_signal\n",
    "\n",
    "def zive_read_df_rpeaks(db_path, recordingId):\n",
    "    file_path = Path(db_path, recordingId + '.json')\n",
    "    with open(file_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    df_rpeaks = pd.json_normalize(data, record_path =['rpeaks'])\n",
    "    return df_rpeaks\n",
    "\n",
    "def zive_read_df_data(file_path, name):\n",
    "    df_data = pd.DataFrame()\n",
    "    path = Path(file_path)\n",
    "    # https://www.askpython.com/python-modules/check-if-file-exists-in-python\n",
    "    if (path.exists()):\n",
    "        # with open(file_path,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        with open(file_path,'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        df_data = pd.json_normalize(data, record_path =[name])\n",
    "    return df_data\n",
    "\n",
    "def get_filename(rec_dir, SubjCode):\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    if (userNr < 1000):\n",
    "        return str(SubjCode)\n",
    "\n",
    "    # Patikriname, ar df_transl egzistuoja. \n",
    "    file = Path(rec_dir, 'df_transl.csv')\n",
    "    if (file.exists()):\n",
    "        # Nuskaitome vardų žodyną iš rec_dir aplanko\n",
    "        file_path = Path(rec_dir, 'df_transl.csv')\n",
    "        df_transl = pd.read_csv(file_path, index_col=0)\n",
    "#       print(df_transl) \n",
    "         # Panaudodami df masyvą df_transl su įrašų numeriais iš įrašų eilės numerių gauname ZIVE numerius\n",
    "        row = df_transl.loc[(df_transl['userNr'] == userNr) & (df_transl['recordingNr'] == recNr)]\n",
    "        if row.empty:\n",
    "            print(\"Klaida!\")\n",
    "            return None\n",
    "        else:\n",
    "            return row['file_name'].values[0]  # naudojame values[0], nes gauname series elementą\n",
    "    else:\n",
    "        print(\"df_transl neegzistuoja\")\n",
    "\n",
    "def split_SubjCode(SubjCode):\n",
    "    \"\"\"\n",
    "    Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "    \n",
    "    zive atveju: SubjCode = int(str(userNr) + str(registrationNr)), kur userNr >= 1000,\n",
    "    pvz. SubjCode = 10001\n",
    "    mit2zive atveju: SubjCode = userNr,  kur userNr < 1000,\n",
    "    pvz. SubjCode = 101\n",
    "    https://www.adamsmith.haus/python/answers/how-to-get-the-part-of-a-string-before-a-specific-character-in-python\n",
    "    Parameters\n",
    "    ------------\n",
    "        SubjCode: int\n",
    "    Return\n",
    "    -----------\n",
    "        userNr: int\n",
    "        recordingNr: int\n",
    "    \"\"\"   \n",
    "    if (SubjCode < 1000):\n",
    "        userNr = SubjCode\n",
    "        recordingNr = 0   \n",
    "        return userNr, recordingNr\n",
    "    else:        \n",
    "        str_code = str(SubjCode) \n",
    "        chars = list(str_code)\n",
    "        str1 =\"\"\n",
    "        userNr = int(str1.join(chars[:4]))\n",
    "        str2 =\"\"\n",
    "        recordingNr = int(str2.join(chars[4:]))\n",
    "        return userNr, recordingNr\n",
    " \n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# file_names = ['1625402.027']\n",
    "# SubjCodes = [10021, 10022, 10083, 10091] #Testavimui, iš mokymo imties\n",
    "SubjCodes = [10083] #Testavimui, iš mokymo imties\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "# I-a dalis: nuskaitome ir analizuojame EKG įrašo anotacijas\n",
    "\n",
    "    \n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    print(\"\\nZive įrašas:\")\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"SubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2}\")\n",
    "    \n",
    "    filepath = Path(rec_dir, str(SubjCode) + '.json')\n",
    "    with open(filepath,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        data = json.loads(f.read())\n",
    "    userId = data['userId']\n",
    "    recordingId = data['recordingId']\n",
    "    print(f\"userId: {userId}  recordingId: {recordingId}\")\n",
    "\n",
    "    print(\"\\nNuskaitome ir analizuojame EKG įrašo anotacijas\")\n",
    "\n",
    "     # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks_json = zive_read_df_rpeaks(rec_dir, str(SubjCode))\n",
    "    atr_sample = df_rpeaks_json['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks_json['annotationValue'].to_numpy()\n",
    "    # print(\"\\natr_sample:\")\n",
    "    # print(atr_sample)\n",
    "    # print(\"\\ndf_rpeaks_json:\")\n",
    "    # print(df_rpeaks_json)\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "    \n",
    "    (unique, counts) = np.unique(atr_symbol, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_symbols: \", unique, counts, total)\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "    # print(f\"df_rpeaks length: {len(df_rpeaks)} \")\n",
    "\n",
    "\n",
    "# Suformuojame pūpsnių atributų failą, analogišką, kaip formuoja Zive\n",
    "\n",
    "# Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "    # print(signal.shape)\n",
    "    # print(signal[:100])\n",
    "    ecg_signal_df1 = pd.DataFrame(signal, columns=['orig'])\n",
    "    # print(ecg_signal_df1.head())\n",
    "# _, rpeaks = nk.ecg_peaks(ecg_signal_df['orig'], sampling_rate=sampling_rate, correct_artifacts=False)\n",
    "# https://neurokit2.readthedocs.io/en/latest/_modules/neurokit2/ecg/ecg_peaks.html\n",
    "\n",
    "# Nuoroda į aplanką su originaliais EKG įrašais ir anotacijomis (.json)\n",
    "    rec_dir_org = Path(db_path, 'records_selected')\n",
    "    filepath = Path(rec_dir_org, str(file_name))\n",
    "    print(filepath)\n",
    "\n",
    "    ecg_signal_df = pd.DataFrame(zive_read_file_1ch(filepath), columns=['orig'])\n",
    "    # zive_read_file_1ch(filepath)\n",
    "    # print(ecg_signal_df.head())\n",
    "\n",
    "\n",
    "    signals, info = nk.ecg_peaks(ecg_signal_df['orig'], sampling_rate=200, correct_artifacts=False)\n",
    "    # _, rpeaks = nk.ecg_peaks(ecg_signal_df['orig'], sampling_rate=200, correct_artifacts=False)\n",
    "    # print(\"\\rpeaks iš neurokit2:\")\n",
    "    # print(info)\n",
    "\n",
    "\n",
    "# II-a dalis: nuskaitome ir analizuojame klasifikacijos rezultatus\n",
    "    print(\"\\nNuskaitome ir analizuojame klasifikacijos rezultatus\")\n",
    "    file_path = Path(res_dir, str(file_name))\n",
    "    print(f\"file_path: {file_path}\")\n",
    "\n",
    "    df_rpeaks_docker = zive_read_df_data(file_path, 'automatic_classification')\n",
    "    # print(f\"df_rpeaks length: {len(df_rpeaks)} \")\n",
    "    # print(df_rpeaks_docker)\n",
    "    res_sample = df_rpeaks_docker['sample'].to_numpy()\n",
    "    res_symbol = df_rpeaks_docker['annotation'].to_numpy()\n",
    "    # print(res_symbol.dtype)\n",
    "\n",
    "    # print(f\"\\natr_sample: {atr_sample[:40]}\")\n",
    "    # print(f\"\\natr_symbol: {atr_symbol[:40]}\")\n",
    "    # print(f\"\\nres_sample: {res_sample[:40]}\")\n",
    "    # print(f\"\\nres_symbol: {res_symbol[:40]}\")\n",
    "\n",
    "    pred_symbol = np.empty(len(atr_sample), dtype='object')\n",
    "    # print(pred_symbol.dtype)\n",
    "\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        symbol = 'U'\n",
    "        for j, j_sample in enumerate(res_sample):\n",
    "            if (atr_sample[i] == res_sample[j]):\n",
    "                symbol = res_symbol[j]\n",
    "        pred_symbol[i] = symbol\n",
    "\n",
    "\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    pred_labels = np.array([all_beats[symbol] for symbol in pred_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(pred_symbol, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_symbols: \", unique, counts, total)\n",
    "\n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \", unique, counts, total)\n",
    "\n",
    "    symbol_dict = dict(zip(all_beats.values(), all_beats.keys()))\n",
    "    # print(symbol_dict)\n",
    "\n",
    "     # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':symbol_dict[test_labels[i]], 'pred':symbol_dict[pred_labels[i]]})\n",
    "            # classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "     # print(classification)\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        tot = 0\n",
    "        tot_U = 0\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")\n",
    "            if (row['pred'] != 'U'):\n",
    "                tot += 1\n",
    "            else:\n",
    "                tot_U +=1     \n",
    "\n",
    "\n",
    "# Išvedame lygiagrečiai iš json ir klasifikacijos\n",
    "    len_json = len(df_rpeaks_json)\n",
    "    len_docker = len(df_rpeaks_docker)\n",
    "    length = len_json\n",
    "    if (len_docker > length):\n",
    "        length = len_docker\n",
    "\n",
    "    # print(df_rpeaks_json)\n",
    "    # print(df_rpeaks_docker)\n",
    "    \n",
    "    print(f\"i,     json          docker\")\n",
    "\n",
    "    for i in range(length):\n",
    "        if (i < len_json):\n",
    "            sampleIndex = df_rpeaks_json.loc[i, 'sampleIndex']\n",
    "            annotationValue = df_rpeaks_json.loc[i, 'annotationValue']\n",
    "        else:\n",
    "            sampleIndex = 0\n",
    "            annotationValue = 0\n",
    "        \n",
    "        if (i < len_docker):\n",
    "            sample = df_rpeaks_docker.loc[i, 'sample']\n",
    "            annotation = df_rpeaks_docker.loc[i, 'annotation']\n",
    "        else:\n",
    "            sample = 0\n",
    "            annotation = 0\n",
    "\n",
    "        print(f\"{i:>3}  {sampleIndex:>6}  {annotationValue:>3}  {sample:>9}  {annotation:>3}\")                 \n",
    "\n",
    "    res_json = df_rpeaks_json['sampleIndex'].to_numpy()\n",
    "    res_docker = df_rpeaks_docker['sample'].to_numpy()\n",
    "\n",
    "    sutampa = len(np.intersect1d(res_json, res_docker))\n",
    "    print(f\"\\nSutampa rpeaks: {sutampa} iš {length}\")\n",
    "\n",
    "    print(\"\\nUnique values in res_json that are not in res_docker:\")\n",
    "    print(np.setdiff1d(res_json, res_docker))\n",
    "\n",
    "    print(\"\\nUnique values in res_docker that are not in res_json:\")\n",
    "    print(np.setdiff1d(res_docker, res_json))\n",
    "\n",
    "    # print(f\"rpeaks total: {len(atr_sample)}\")\n",
    "    # print(f\"excluded rpeaks ('U'): {tot_U}\")\n",
    "    # print(f\"classified rpeaks: {len(atr_sample) - tot_U}\")\n",
    "    # print(f\"automaticRpeakAnnotations: {tot}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
