{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skriptas Zive klasifikavimo rezultatų failo analizei\n",
      "OS in my system :  linux\n",
      "\n",
      "Bendras duomenų aplankas:  /home/kesju/DI/Data/MIT&ZIVE/VU\n",
      "Zive duomenų aplankas:  DUOM_VU\n",
      "Aplankas su EKG įrašais:  /home/kesju/DI/Data/MIT&ZIVE/VU/DUOM_VU/records_npy\n",
      "Aplankas su resultatais:  /home/kesju/DI/Data/MIT&ZIVE/VU/DUOM_VU/test_results/test\n",
      "Pūpsnių atributų failas: all_beats_attr_z.csv\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variantas:  vieno įrašo detalus testavimas \n",
    "\n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris arba paimamas if failo info_create.json,\n",
    "# arba iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "from icecream import ic\n",
    "\n",
    "# from zive_util_vu import cm2df, show_confusion_matrix \n",
    "# from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "# from zive_util_vu import runtime, split_SubjCode\n",
    "# from zive_util_vu import get_freq_unique_values\n",
    "\n",
    "# from zive_util_vu import get_beat_attributes\n",
    "# from zive_util_vu import get_userId, read_rec, get_filename \n",
    "# from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "# from zive_cnn_fda_vu_v1 import predict_cnn_fda_vu_v1, zive_read_file_1ch\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "\n",
    "print(\"Skriptas Zive klasifikavimo rezultatų failo analizei\")\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3}  \n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "\n",
    "res_dir = Path(db_path, 'test_results', 'test')\n",
    "# res_dir = '/home/test_results/test'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su EKG įrašais: \", rec_dir)\n",
    "print(\"Aplankas su resultatais: \", res_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasifikuojamų įrašų sąrašas: [10021]\n",
      "\n",
      "Zive įrašas:\n",
      "SubjCode: 10021 userNr: 1002 file_name: 1625402.027\n",
      "userId: 60e1d80f93b55b41529e9eaa  recordingId: 60e1db5a93b55b56919e9ec0\n",
      "\n",
      "Nuskaitome ir analizuojame EKG įrašo anotacijas\n",
      "test_symbols:  ['N' 'S' 'V'] [748   4  13] 765\n",
      "test_labels:  [0 1 2] [748   4  13] 765\n",
      "\n",
      "Nuskaitome ir analizuojame klasifikacijos rezultatus\n",
      "file_path: /home/kesju/DI/Data/MIT&ZIVE/VU/DUOM_VU/test_results/test/1625402.027\n",
      "   sample annotation\n",
      "0     188          U\n",
      "1     346          N\n",
      "2     507          N\n",
      "3     667          N\n",
      "4     828          N\n",
      "\n",
      "res_sample: [ 188  346  507  667  828  985 1151 1323 1495 1679 1858 2041 2219 2392\n",
      " 2557 2725 2895 3074 3253 3439 3618 3786 3950 4111 4275 4438 4605 4773\n",
      " 4940 5111 5282 5449 5610 5770 5932 6095 6263 6433 6611 6802]\n",
      "\n",
      "res_symbol: ['U' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N']\n",
      "\n",
      "atr_sample: [ 188  346  507  667  828  985 1151 1323 1495 1679 1858 2042 2219 2392\n",
      " 2557 2725 2895 3074 3253 3439 3618 3786 3950 4111 4275 4438 4605 4773\n",
      " 4940 5111 5282 5449 5610 5770 5932 6095 6263 6433 6611 6802]\n",
      "\n",
      "atr_symbol: ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N']\n",
      "pred_symbols:  ['N' 'S' 'U' 'V'] [739   7   6  13] 765\n",
      "pred_labels:  [0 1 2 3] [739   7  13   6] 765\n",
      "sample:     188   annot_label:  N   pred_label:  U\n",
      "sample:    2042   annot_label:  N   pred_label:  U\n",
      "sample:   12819   annot_label:  N   pred_label:  U\n",
      "sample:   13072   annot_label:  N   pred_label:  S\n",
      "sample:   13188   annot_label:  N   pred_label:  S\n",
      "sample:   14476   annot_label:  V   pred_label:  V\n",
      "sample:   20055   annot_label:  N   pred_label:  U\n",
      "sample:   20128   annot_label:  N   pred_label:  S\n",
      "sample:   24717   annot_label:  V   pred_label:  V\n",
      "sample:   25942   annot_label:  V   pred_label:  V\n",
      "sample:   27530   annot_label:  V   pred_label:  U\n",
      "sample:   28427   annot_label:  S   pred_label:  S\n",
      "sample:   28694   annot_label:  S   pred_label:  S\n",
      "sample:   29147   annot_label:  V   pred_label:  V\n",
      "sample:   38814   annot_label:  V   pred_label:  V\n",
      "sample:   69512   annot_label:  V   pred_label:  V\n",
      "sample:   78648   annot_label:  S   pred_label:  S\n",
      "sample:   78823   annot_label:  N   pred_label:  V\n",
      "sample:   78916   annot_label:  S   pred_label:  S\n",
      "sample:   90434   annot_label:  V   pred_label:  V\n",
      "sample:   91755   annot_label:  V   pred_label:  V\n",
      "sample:   95532   annot_label:  V   pred_label:  V\n",
      "sample:   97118   annot_label:  V   pred_label:  V\n",
      "sample:   98659   annot_label:  V   pred_label:  V\n",
      "sample:  116536   annot_label:  V   pred_label:  V\n",
      "sample:  127878   annot_label:  N   pred_label:  U\n",
      "rpeaks total: 765\n",
      "excluded rpeaks ('U'): 6\n",
      "classified rpeaks: 759\n",
      "automaticRpeakAnnotations: 20\n"
     ]
    }
   ],
   "source": [
    "def zive_read_df_rpeaks(db_path, recordingId):\n",
    "    file_path = Path(db_path, recordingId + '.json')\n",
    "    with open(file_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    df_rpeaks = pd.json_normalize(data, record_path =['rpeaks'])\n",
    "    return df_rpeaks\n",
    "\n",
    "def zive_read_df_data(file_path, name):\n",
    "    df_data = pd.DataFrame()\n",
    "    path = Path(file_path)\n",
    "    # https://www.askpython.com/python-modules/check-if-file-exists-in-python\n",
    "    if (path.exists()):\n",
    "        # with open(file_path,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        with open(file_path,'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        df_data = pd.json_normalize(data, record_path =[name])\n",
    "    return df_data\n",
    "\n",
    "def get_filename(rec_dir, SubjCode):\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    if (userNr < 1000):\n",
    "        return str(SubjCode)\n",
    "\n",
    "    # Patikriname, ar df_transl egzistuoja. \n",
    "    file = Path(rec_dir, 'df_transl.csv')\n",
    "    if (file.exists()):\n",
    "        # Nuskaitome vardų žodyną iš rec_dir aplanko\n",
    "        file_path = Path(rec_dir, 'df_transl.csv')\n",
    "        df_transl = pd.read_csv(file_path, index_col=0)\n",
    "#       print(df_transl) \n",
    "         # Panaudodami df masyvą df_transl su įrašų numeriais iš įrašų eilės numerių gauname ZIVE numerius\n",
    "        row = df_transl.loc[(df_transl['userNr'] == userNr) & (df_transl['recordingNr'] == recNr)]\n",
    "        if row.empty:\n",
    "            print(\"Klaida!\")\n",
    "            return None\n",
    "        else:\n",
    "            return row['file_name'].values[0]  # naudojame values[0], nes gauname series elementą\n",
    "    else:\n",
    "        print(\"df_transl neegzistuoja\")\n",
    "\n",
    "def split_SubjCode(SubjCode):\n",
    "    \"\"\"\n",
    "    Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "    \n",
    "    zive atveju: SubjCode = int(str(userNr) + str(registrationNr)), kur userNr >= 1000,\n",
    "    pvz. SubjCode = 10001\n",
    "    mit2zive atveju: SubjCode = userNr,  kur userNr < 1000,\n",
    "    pvz. SubjCode = 101\n",
    "    https://www.adamsmith.haus/python/answers/how-to-get-the-part-of-a-string-before-a-specific-character-in-python\n",
    "    Parameters\n",
    "    ------------\n",
    "        SubjCode: int\n",
    "    Return\n",
    "    -----------\n",
    "        userNr: int\n",
    "        recordingNr: int\n",
    "    \"\"\"   \n",
    "    if (SubjCode < 1000):\n",
    "        userNr = SubjCode\n",
    "        recordingNr = 0   \n",
    "        return userNr, recordingNr\n",
    "    else:        \n",
    "        str_code = str(SubjCode) \n",
    "        chars = list(str_code)\n",
    "        str1 =\"\"\n",
    "        userNr = int(str1.join(chars[:4]))\n",
    "        str2 =\"\"\n",
    "        recordingNr = int(str2.join(chars[4:]))\n",
    "        return userNr, recordingNr\n",
    " \n",
    "\n",
    "\n",
    "# file_names = ['1625402.027']\n",
    "# SubjCodes = [10021, 10022, 10083, 10091] #Testavimui, iš mokymo imties\n",
    "SubjCodes = [10021] #Testavimui, iš mokymo imties\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "# I-a dalis: nuskaitome ir analizuojame EKG įrašo anotacijas\n",
    "\n",
    "    \n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    print(\"\\nZive įrašas:\")\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"SubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2}\")\n",
    "    \n",
    "    filepath = Path(rec_dir, str(SubjCode) + '.json')\n",
    "    with open(filepath,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        data = json.loads(f.read())\n",
    "    userId = data['userId']\n",
    "    recordingId = data['recordingId']\n",
    "    print(f\"userId: {userId}  recordingId: {recordingId}\")\n",
    "\n",
    "    print(\"\\nNuskaitome ir analizuojame EKG įrašo anotacijas\")\n",
    "\n",
    "     # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = zive_read_df_rpeaks(rec_dir, str(SubjCode))\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "    \n",
    "    (unique, counts) = np.unique(atr_symbol, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_symbols: \", unique, counts, total)\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "    # print(f\"df_rpeaks length: {len(df_rpeaks)} \")\n",
    "\n",
    "\n",
    "# II-a dalis: nuskaitome ir analizuojame klasifikacijos rezultatus\n",
    "    print(\"\\nNuskaitome ir analizuojame klasifikacijos rezultatus\")\n",
    "    file_path = Path(res_dir, str(file_name))\n",
    "    print(f\"file_path: {file_path}\")\n",
    "\n",
    "    df_rpeaks = zive_read_df_data(file_path, 'automatic_classification')\n",
    "    # print(f\"df_rpeaks length: {len(df_rpeaks)} \")\n",
    "    print(df_rpeaks.head())\n",
    "    res_sample = df_rpeaks['sample'].to_numpy()\n",
    "    res_symbol = df_rpeaks['annotation'].to_numpy()\n",
    "    # print(res_symbol.dtype)\n",
    "\n",
    "    print(f\"\\nres_sample: {res_sample[:40]}\")\n",
    "    print(f\"\\nres_symbol: {res_symbol[:40]}\")\n",
    "    print(f\"\\natr_sample: {atr_sample[:40]}\")\n",
    "    print(f\"\\natr_symbol: {atr_symbol[:40]}\")\n",
    "    \n",
    "\n",
    "    pred_symbol = np.empty(len(atr_sample), dtype='object')\n",
    "    # print(pred_symbol.dtype)\n",
    "\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        symbol = 'U'\n",
    "        for j, j_sample in enumerate(res_sample):\n",
    "            if (atr_sample[i] == res_sample[j]):\n",
    "                symbol = res_symbol[j]\n",
    "        pred_symbol[i] = symbol\n",
    "\n",
    "\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    pred_labels = np.array([all_beats[symbol] for symbol in pred_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(pred_symbol, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_symbols: \", unique, counts, total)\n",
    "\n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \", unique, counts, total)\n",
    "\n",
    "    symbol_dict = dict(zip(all_beats.values(), all_beats.keys()))\n",
    "    # print(symbol_dict)\n",
    "\n",
    "     # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':symbol_dict[test_labels[i]], 'pred':symbol_dict[pred_labels[i]]})\n",
    "            # classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "     # print(classification)\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        tot = 0\n",
    "        tot_U = 0\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")\n",
    "            if (row['pred'] != 'U'):\n",
    "                tot += 1\n",
    "            else:\n",
    "                tot_U +=1     \n",
    "\n",
    "    print(f\"rpeaks total: {len(atr_sample)}\")\n",
    "    print(f\"excluded rpeaks ('U'): {tot_U}\")\n",
    "    print(f\"classified rpeaks: {len(atr_sample) - tot_U}\")\n",
    "    print(f\"automaticRpeakAnnotations: {tot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atliekama pūpsnių pacientų įrašuose klasifikacija\n",
      "Klasifikuojamų įrašų sąrašas: [10021, 10022, 10083, 10091]\n",
      "Sąrašas nuskaitytas iš: testinis_sarasas.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_rec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12128\\620752917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Nuskaitome EKG įrašą (npy formatu)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0msign_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_rec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrec_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubjCode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0msignal_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msign_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msignal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msign_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_rec' is not defined"
     ]
    }
   ],
   "source": [
    "# PASIRUOŠIMAS\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Naudojamų požymių sąrašas\n",
    "\n",
    "               \n",
    "print(\"\\nAtliekama pūpsnių pacientų įrašuose klasifikacija\")\n",
    "\n",
    "# Pacientų įrašų sąrašas testavimui\n",
    "file_path = 'testinis_sarasas.csv'\n",
    "SubjCodes = [10021, 10022, 10083, 10091] #Testavimui, iš mokymo imties\n",
    "# SubjCodes = [10021] #Testavimuas pacientų įrašų sąrašas\n",
    "\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "print(f\"Sąrašas nuskaitytas iš: {file_path}\")\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "# print(class_names)\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(rec_dir, all_beats_attr_fname)\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'recordingNr': int,\n",
    "                                                             'sample': int, 'symbol': str, 'label': int})\n",
    "all_beat_indices = all_beats_attr.index\n",
    "\n",
    "index_start = 0\n",
    "# Sukūriame masyvą, į kurį sudėsime visų įrašų pūpsnių anotuotus ir automatiškai surastus klasių numerius\n",
    "validation_set_stats = pd.DataFrame(columns=['idx', 'test_label', 'pred_label', 'SubjCode'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Ciklas per pacientų įrašus\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "    # Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    print(\"\\nZive įrašas:\")\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"SubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} signal_length: {signal_length}\")\n",
    "    \n",
    "    filepath = Path(rec_dir, str(SubjCode) + '.json')\n",
    "    with open(filepath,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "        data = json.loads(f.read())\n",
    "    userId = data['userId']\n",
    "    recordingId = data['recordingId']\n",
    "    print(f\"userId: {userId}  recordingId: {recordingId}\")\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = zive_read_df_rpeaks(rec_dir, str(SubjCode))\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(atr_symbol, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_symbols: \", unique, counts, total)\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "   \n",
    "    pred_labels = predict_cnn_fda_vu_v1(signal, atr_sample, model_dir)\n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels, praleisti (šiuo atveju pirmas\n",
    "    # ir paskutinis pūpsnys), o taip pat pakliuvęs į ommited sritį, pažymimi klase 3\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "\n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \",unique, counts, total)\n",
    "\n",
    "    symbol_dict = dict(zip(all_beats.values(), all_beats.keys()))\n",
    "    print(symbol_dict)\n",
    "    # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':symbol_dict[test_labels[i]], 'pred':symbol_dict[pred_labels[i]]})\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # Dirbant su daug įrašų sąrašo išvedimą reikia užblokuoti !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        tot = 0\n",
    "        tot_U = 0\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")\n",
    "            if (row['pred'] != 'U'):\n",
    "                tot += 1\n",
    "            else:\n",
    "                tot_U +=1     \n",
    "\n",
    "    print(f\"rpeaks total: {len(atr_sample)}\")\n",
    "    print(f\"excluded rpeaks ('U'): {tot_U}\")\n",
    "    print(f\"classified rpeaks: {len(atr_sample) - tot_U}\")\n",
    "    print(f\"automaticRpeakAnnotations: {tot}\")\n",
    "\n",
    "    # SUFORMUOJAME FREIMĄ validation_set_stats SU PŪPSNIŲ KLASIŲ ANOTUOTAIS IR AUTOMATIŠKAI \n",
    "    # SURASTAIS KLASIŲ NUMERIAIS, IŠMETANT KLASES SU NUMERIU = 3\n",
    "\n",
    "    # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "    selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['recordingNr']==recNr)]\n",
    "    # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "    index_start = selected_ind[0]\n",
    "    # print('\\nSubjCode:',SubjCode, 'index_start:', index_start)   \n",
    "\n",
    "    #  Praleisdami indeksą, jei masyvuose test_labels ir pred_labels yra reikšmė == 3,\n",
    "    # suformuojame klasifikuotinų pūpsnių indeksų sąrašą\n",
    "    for idx in range(len(atr_sample)):\n",
    "        flag = (test_labels[idx] == 3) or (pred_labels[idx] == 3)\n",
    "        if (flag == False):\n",
    "            # Dėmesio: ?????????????????????????????????????????????\n",
    "            # taisytina vieta, bus problemų su pandas 1.4.1\n",
    "            validation_set_stats = validation_set_stats.append({'idx':index_start+idx, \n",
    "            'test_label':test_labels[idx],'pred_label':pred_labels[idx], 'SubjCode': SubjCode}, ignore_index=True)\n",
    "\n",
    "            # validation_set_stats = validation_set_stats.append({'idx':index_start+idx, \n",
    "            # 'test_label':test_labels[idx],'pred_label':pred_labels[idx], 'SubjCode': SubjCode}, ignore_index=True)\n",
    "\n",
    "    # Suformuojame klasių numerių msyvus confusion matricai skaičiuoti, surandama confusion matrica\n",
    "    test_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['test_label']).astype('int') \n",
    "    # print(all_beats_attr.info())\n",
    "    pred_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['pred_label']).astype('int')\n",
    "   \n",
    "    # Atsikračius pūpsnių su klase = 3 ir suformavus masyvus, pred_y turi būti tokio pat ilgio, kaip ir test_y\n",
    "    if (len(test_y) != len(pred_y)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_y ir pred_y ilgiai\")     \n",
    "\n",
    "    # Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "    confusion = confusion_matrix(test_y, pred_y)\n",
    "    # print(confusion)\n",
    "    pd.set_option('display.precision',3)\n",
    "    show_confusion_matrix(confusion, class_names)\n",
    "\n",
    "# print('\\n')\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_y, pred_y, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(sup[0]):>5} S:{(int(sup[1])):3} V:{int(sup[2]):3}\" \n",
    "    str2 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str3 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str4 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "    # print(len(validation_set_stats))\n",
    "    # print(len(test_y))\n",
    "    # print(len(pred_y))\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "# Sukūriame anotuotų ir automatiškai priskirtų klasių visų įrašų pūpsniams sąrašus \n",
    "validate_ind_lst = list(validation_set_stats['idx'])\n",
    "y_validate = np.array(validation_set_stats['test_label']).astype('int')\n",
    "y_predicted = np.array(validation_set_stats['pred_label']).astype('int')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
