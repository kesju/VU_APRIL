{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris arba paimamas if failo info_create.json,\n",
    "# arba iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os, json\n",
    "from pathlib import Path\n",
    "from icecream import ic\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from zive_util_vu import cm2df, show_confusion_matrix \n",
    "from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "from zive_util_vu import runtime, split_SubjCode\n",
    "from zive_util_vu import get_freq_unique_values\n",
    "\n",
    "from zive_util_vu import get_beat_attributes\n",
    "from zive_util_vu import get_userId, read_rec, get_filename \n",
    "from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "from zive_cnn_fda_vu_v1 import predict_cnn_fda_vu_v1, zive_read_file_1ch\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\")\n",
    "print('Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio')\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3, 'F':3}  \n",
    "\n",
    "# Diskretizavimo dažnis:\n",
    "fs = 200\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į modelio aplanką\n",
    "# model_dir = Path(Duomenu_aplankas, 'DNN', 'best_models', 'all_ft')\n",
    "model_dir = 'model_cnn_fda_vu_v1'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su originaliais EKG įrašais ir anotacijomis (.json) \", rec_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "print(\"Modelio ir scaler parametrai nuskaitomas iš aplanko: \", model_dir)\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASIRUOŠIMAS\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Naudojamų požymių sąrašas\n",
    "\n",
    "all_features = ['RRl', 'RRr', 'RRl/RRr',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "\n",
    "print(\"\\nAtliekama pūpsnių pacientų įrašuose klasifikacija\")\n",
    "\n",
    "# NURODOME PACIENTŲ SĄRAŠĄ. GALIMI ĮVAIRŪS VARIANTAI\n",
    "\n",
    "# Variantas: visi duomenys\n",
    "# Nuskaitome failą info_create.json ir duomenų rinkinio parametrus\n",
    "file_path = Path(rec_dir,'info_create_z.json')\n",
    "with open(file_path) as json_file:\n",
    "    info_create = json.load(json_file)\n",
    "# SubjCodes =  info_create['SubjCodes'] # pacientų įrašų sąrašas\n",
    "\n",
    "# Variantas: mokymo imtis  \n",
    "# file_path = Path(rec_dir, 'train_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: validation imtis  \n",
    "# file_path = Path(rec_dir, 'validation_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: testinė imtis  \n",
    "file_path = Path(rec_dir, 'test_subjcode_lst.csv')\n",
    "SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Pacientų įrašų sąrašas testavimui\n",
    "# file_path = 'Testinis sąrašas'\n",
    "# SubjCodes = [10020, 10021, 10051] #Testavimui\n",
    "\n",
    "# Išvedamas pacientų įrašų sąrašas\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "print(f\"Sąrašas nuskaitytas iš: {file_path}\")\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "print(class_names)\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(rec_dir, all_beats_attr_fname)\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'recordingNr': int,\n",
    "                                                             'sample': int, 'symbol': str, 'label': int})\n",
    "all_beat_indices = all_beats_attr.index\n",
    "\n",
    "index_start = 0\n",
    "# Sukūriame masyvą, į kurį sudėsime visų įrašų pūpsnių anotuotus ir automatiškai surastus klasių numerius\n",
    "validation_set_stats = pd.DataFrame(columns=['idx', 'test_label', 'pred_label', 'SubjCode'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Ciklas per pacientų įrašus\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "    # Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} signal_length: {signal_length}\")\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = zive_read_df_rpeaks(rec_dir, str(SubjCode))\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "   \n",
    "    pred_labels = predict_cnn_fda_vu_v1(signal, atr_sample, model_dir)\n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels, praleisti (šiuo atveju pirmas\n",
    "    # ir paskutinis pūpsnys), o taip pat pakliuvęs į ommited sritį, pažymimi klase 3\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "\n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \",unique, counts, total)\n",
    "\n",
    "    # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # Dirbant su daug įrašų sąrašo išvedimą reikia užblokuoti !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")  \n",
    "    \n",
    "    # SUFORMUOJAME FREIMĄ validation_set_stats SU PŪPSNIŲ KLASIŲ ANOTUOTAIS IR AUTOMATIŠKAI \n",
    "    # SURASTAIS KLASIŲ NUMERIAIS, IŠMETANT KLASES SU NUMERIU = 3\n",
    "\n",
    "    # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "    selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['recordingNr']==recNr)]\n",
    "    # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "    index_start = selected_ind[0]\n",
    "    # print('\\nSubjCode:',SubjCode, 'index_start:', index_start)   \n",
    "\n",
    "    #  Praleisdami indeksą, jei masyvuose test_labels ir pred_labels yra reikšmė == 3,\n",
    "    # suformuojame klasifikuotinų pūpsnių indeksų sąrašą\n",
    "    for idx in range(len(atr_sample)):\n",
    "        flag = (test_labels[idx] == 3) or (pred_labels[idx] == 3)\n",
    "        if (flag == False):\n",
    "            # Dėmesio: ?????????????????????????????????????????????\n",
    "            # taisytina vieta, bus problemų su pandas 1.4.1\n",
    "            validation_set_stats = validation_set_stats.append({'idx':index_start+idx, \n",
    "            'test_label':test_labels[idx],'pred_label':pred_labels[idx], 'SubjCode': SubjCode}, ignore_index=True)\n",
    "\n",
    "    # Suformuojame klasių numerių msyvus confusion matricai skaičiuoti, surandama confusion matrica\n",
    "    test_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['test_label']).astype('int') \n",
    "    # print(all_beats_attr.info())\n",
    "    pred_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['pred_label']).astype('int')\n",
    "   \n",
    "    # Atsikračius pūpsnių su klase = 3 ir suformavus masyvus, pred_y turi būti tokio pat ilgio, kaip ir test_y\n",
    "    if (len(test_y) != len(pred_y)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_y ir pred_y ilgiai\")     \n",
    "\n",
    "    confusion = confusion_matrix(test_y, pred_y)\n",
    "    # print(confusion)\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_y, pred_y, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(sup[0]):>5} S:{(int(sup[1])):3} V:{int(sup[2]):3}\" \n",
    "    str2 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str3 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str4 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "    # print(len(validation_set_stats))\n",
    "    # print(len(test_y))\n",
    "    # print(len(pred_y))\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "# Sukūriame anotuotų ir automatiškai priskirtų klasių visų įrašų pūpsniams sąrašus \n",
    "validate_ind_lst = list(validation_set_stats['idx'])\n",
    "y_validate = np.array(validation_set_stats['test_label']).astype('int')\n",
    "y_predicted = np.array(validation_set_stats['pred_label']).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELIO TIKSLUMO VERTINIMO IŠ VERTINIMO IMTIES REZULTATAI\n",
    "\n",
    "print(\"\\nMODELIO TIKSLUMO VERTINIMO REZULTATAI\")\n",
    "print(\"Modelis iš aplanko: \", model_dir)\n",
    "\n",
    "cols, dist, tot = get_freq_unique_values(y_validate, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot} \")\n",
    "\n",
    "# APIBENDRINTI REZULTATAI\n",
    "\n",
    "print('\\nAPIBENDRINTI REZULTATAI\\n')\n",
    "\n",
    "# Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "confusion = confusion_matrix(y_validate, y_predicted)\n",
    "pd.set_option('display.precision',3)\n",
    "show_confusion_matrix(confusion, class_names)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"\\nClassification Report\\n\")\n",
    "# target_names = [key for (key, value) in selected_beats.items()]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(classification_report(y_validate, y_predicted, target_names=class_names, digits=3))\n",
    "report = classification_report(y_validate, y_predicted, target_names=class_names, output_dict=True)\n",
    "# output_dictbool, default=False, If True, return output as dict.\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# https://medium.com/@asmaiya/you-can-something-like-this-84d28e0fd31f\n",
    "\n",
    "# Įrašome į diską\n",
    "filepath = Path(model_dir, 'Apibendrinti_rezultatai.csv') \n",
    "df_report.to_csv(filepath)    \n",
    "print(f'\\nApibendrinti_rezultatai įrašyti į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLAIDŲ PASISKIRSTYMAS PER PACIENTUS IR JŲ ĮRAŠUS\n",
    "\n",
    "from zive_util_vu import zive_read_df_data, create_SubjCode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def collect_noise_locs(rec_dir, all_beats_attr):\n",
    "\n",
    "    noise_arr_tot = np.empty(0, dtype=int)\n",
    "    \n",
    "    grouped = all_beats_attr.groupby(['userNr', 'recordingNr'])\n",
    "    for key, group in grouped:\n",
    "        # print('\\n',key)\n",
    "        userNr = key[0]\n",
    "        recordingNr = key[1]\n",
    "\n",
    "        # SubjCode naudojamas atveju, kai duomenis yra formoje SubjCode.npy, SubjCode.json\n",
    "        SubjCode = create_SubjCode(userNr, recordingNr)\n",
    "        filepath = Path(rec_dir, str(SubjCode) + '.json') \n",
    "        df_noises = zive_read_df_data(filepath, 'noises')\n",
    "\n",
    "        noise_arr = np.full(shape=len(group), fill_value=0,  dtype=int)\n",
    "        idx_noise = 0\n",
    "\n",
    "        for i, row_i in group.iterrows():\n",
    "            sample = row_i['sample']\n",
    "            for j, row_j in df_noises.iterrows():\n",
    "                if ((sample > row_j['startIndex']) & (sample < row_j['endIndex'])):\n",
    "                    noise_arr[idx_noise] = 1\n",
    "                    idx_noise += 1\n",
    "        noise_arr_tot = np.append(noise_arr_tot, noise_arr)\n",
    "    return noise_arr_tot\n",
    "\n",
    "def get_error(y_test, y_pred):\n",
    "    # Error in %\n",
    "    n_errors = 0\n",
    "    for idx in range(len(y_pred)):\n",
    "        if (y_test[idx] != y_pred[idx]):\n",
    "            n_errors += 1\n",
    "    Err = float(n_errors)/len(y_pred)*100.\n",
    "    Err = round(Err, 1)\n",
    "    return Err\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Sukuriame ir užpildome dataframe su sekų parametrais\n",
    "df_seq_errors = pd.DataFrame(columns= ['idx', 'userNr', 'file_name'])\n",
    "\n",
    "rows_list = []\n",
    "for idx in validate_ind_lst:\n",
    "# Surandame  userNr, recordingNr, symbol\n",
    "    userNr, recNr, label, symbol = get_beat_attributes(idx, all_beats_attr)\n",
    "    seq_attr = {'idx': idx, 'userNr':userNr, 'recordingNr':recNr}\n",
    "    rows_list.append(seq_attr)\n",
    "\n",
    "# print(rows_list[:10])\n",
    "\n",
    "# Čia įdedame informaciją, ar pūpsnys patenka į triukšmų zoną\n",
    "noise_arr = collect_noise_locs(rec_dir, all_beats_attr)\n",
    "noise_arr = noise_arr[validate_ind_lst]\n",
    "\n",
    "# print(noise_arr)\n",
    "# unique, counts = np.unique(noise_arr, return_counts=True)\n",
    "# print(unique, counts, noise_arr.shape[0])\n",
    "# print(counts.sum())\n",
    "\n",
    "df_seq_errors = pd.DataFrame(rows_list)\n",
    "\n",
    "df_seq_errors['labels'] = pd.Series(y_validate)\n",
    "df_seq_errors['preds'] = pd.Series(y_predicted)\n",
    "zeros_arr = np.zeros( y_predicted.shape[0], dtype=int)\n",
    "df_seq_errors['errors'] = pd.Series(zeros_arr)  \n",
    "df_seq_errors.loc[df_seq_errors['labels'] != df_seq_errors['preds'], 'errors'] = 1 \n",
    "df_seq_errors['noises'] = pd.Series(noise_arr)  \n",
    "\n",
    "# print(df_seq_errors.info())\n",
    "\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientus\n",
    "\n",
    "print(\"\\nRezultatų pasiskirstymas per pacientus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_user_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'), 'userId':pd.Series(dtype='str'),  # ??????????????????????\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "    'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Išgauname pacientų vidinę (eilės nr) numeraciją\n",
    "grouped  = df_seq_errors.groupby(['userNr'])\n",
    "userNrs = list(grouped.groups.keys())\n",
    "print(f'Pacientų: {len(userNrs)}\\n')\n",
    "\n",
    "for userNr in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[userNr]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[userNr]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "    \n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[userNr]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "    # Testavimui\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    dict_user_errors = {'userNr':int(userNr),'userId':str(userId),   # //////////////////////////////////////////\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err,  'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_user_errors)\n",
    "\n",
    "df_user_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "tit1 = f\"{'userNr':>6} {'userId':>24} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "for idx, row in  df_user_errors.iterrows():\n",
    "    str1 =f\"{int(row['userNr']):>6} {str(row['userId']):>6} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "    str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "    str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "    str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(model_dir, 'Rezultatu_pasiskirstymas_per_pacientus.csv') \n",
    "df_user_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per pacientus įrašytas į:  {filepath}')\n",
    "\n",
    "# print(df_user_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rezultatų pasiskirstymas per pacientus ir jų įrašus\n",
    "# Skaičiuojama visoms 3 klasėms Precision(tikslumas), Recall (atgaminimas), Fscore (F rodiklis)\n",
    "# \n",
    "# https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac\n",
    "# \n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 18)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientų įrašus\n",
    "print(\"\\n\\nRezultatų pasiskirstymas per pacientų įrašus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_rec_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'recordingNr':pd.Series(dtype='int'), 'file_name':pd.Series(dtype='str'),  \n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Sugrupuojame eilutes pagal 'userId','recordingId', suskaičiuojame, kiek kiekviename įraše\n",
    "# iš viso yra klasifikuojamų sekų (labels) ir kiek padaryta klaidų (errors).\n",
    "# Grupavimo objektą paverčiame į normalų dataframe objektą\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "print(f'Pacientų įrašų: {grouped.ngroups}')\n",
    "# print(f'{grouped.size()=}')\n",
    "\n",
    "for key in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    # print(f'\\nGroup: {key}\\n{df_seq_errors.loc[grouped.groups[key]]}')\n",
    "    userNr = key[0]\n",
    "    recordingNr = key[1]\n",
    "    file_name = get_filename(rec_dir, create_SubjCode(userNr, recordingNr))\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[key]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[key]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "\n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[key]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "\n",
    "    # *********************** Testavimui *******************************************************************\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "    # *********************************************************************************************************\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    \n",
    "    dict_rec_errors = {'userNr':int(userNr), 'recordingNr':recordingNr, 'file_name':file_name,\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_rec_errors)\n",
    "\n",
    "df_rec_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "grouped  = df_rec_errors.groupby('userNr')\n",
    "for userNr, group in grouped:\n",
    "    # print(group.dtypes)\n",
    "    print(\"\\n\")\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "    print(f\"{'userNr:'} {userNr} {'userId:'} {userId}\" )\n",
    "    tit1 = f\"{'recordingNr':>6} {'file_name':>15} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "    tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "    tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "    tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "    print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        str1 =f\"{int(row['recordingNr']):>6} {str(row['file_name']):>20} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "        str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "        str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "        str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "        print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(model_dir, 'Rezultatu_pasiskirstymas_per_irasus.csv') \n",
    "df_rec_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per įrašus įrašytas į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesutvarkyta !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Sukuriami užduoto ilgio sekų vaizdai ir įrašomi į disko atitinkamus klasėms subfolderius\n",
    "\n",
    "from zive_util_vu import get_rec_Id, split_seq_file_name, get_symbol_list, get_SubjCode, create_SubjCode\n",
    "from zive_util_vu import read_show_seq_ext_zive, read_seq, get_seq_start_end, read_rec_attrib\n",
    "\n",
    "\n",
    "def read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, win_ls, win_rs, win_ls_ext, win_rs_ext):\n",
    "# Išpjauna užduoto ilgio seką iš mit2zive įrašo ir sukuria jos vaizdą su anotacijomis\n",
    "\n",
    "# rec_dir - paciento EKG įrašų aplankas\n",
    "# recordingId - paciento EKG įrašo Id - int\n",
    "# i_sample - R dantelio, kurio atžvilgiu formuojama seka, indeksas viso EKG įrašo reikšmių masyve - int\n",
    "# win_ls - klasifikuojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs - klasifikuojamo EKG segmento plotis nuo R pūpsnio (iš dešinės)\n",
    "# win_ls_ext - vaizduojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs_ext - vaizduojamo EKG segmento plotis už R pūpsnio (iš dešinės) \n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    sequence, sample, label = read_seq(rec_dir, all_beats_attr, idx, win_ls_ext, win_rs_ext)\n",
    "    if (sample == None):\n",
    "        print(\"klaida!\")\n",
    "        return None\n",
    "    \n",
    "    seq_start = sample - win_ls_ext\n",
    "    seq_end = sample + win_ls_ext\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    atr_sample, atr_symbol = read_rec_attrib(rec_dir, SubjCode)\n",
    "\n",
    "    # # suformuojame anotacijų žymes\n",
    "    beat_symbols,beat_locs = get_symbol_list(atr_symbol,atr_sample, seq_start, seq_end)\n",
    "\n",
    "    # deltax ir deltay simbolių pozicijų koregavimui\n",
    "    min = np.amin(sequence)\n",
    "    max = np.amax(sequence)\n",
    "    deltay = (max - min)/20\n",
    "    deltax = len(sequence)/100\n",
    "\n",
    "    # suformuojame vaizdą\n",
    "    x = np.arange(0, len(sequence), 1)\n",
    "    ax.plot(x, sequence, color=\"#6c3376\", linewidth=2)\n",
    "    left_mark = win_ls_ext - win_ls\n",
    "    right_mark = win_ls_ext + win_rs\n",
    "    ax.axvline(x = left_mark, color = 'b', linestyle = 'dotted')\n",
    "    ax.axvline(x = right_mark, color = 'b', linestyle = 'dotted')\n",
    "    for i in range(len(beat_locs)):\n",
    "        ax.annotate(beat_symbols[i],(beat_locs[i]-deltax,sequence[beat_locs[i]]+deltay))\n",
    "    ax.set_ylim([min, max+2*deltay])\n",
    "    \n",
    "    return(ax)\n",
    "\n",
    "def get_seq_attributes(all_beats_attr, idx):\n",
    "    # 'userNr', 'recordingNr', 'sample', 'symbol', 'label', 'RRl', 'RRr'\n",
    "    row = all_beats_attr.loc[idx]\n",
    "    return list(row) \n",
    "\n",
    "# ////////////////////// Užduodami parametrai /////////////////////\n",
    "\n",
    "# Bendras aplankas vaizdams\n",
    "images_folder = 'CNN'\n",
    "\n",
    "# užduodame, kiek reikšmių vaizduosime prieš R dantelį ir po\n",
    "wl_side_ext = 360\n",
    "wr_side_ext = 360\n",
    "\n",
    "# Užduodame, kiek sekų vaizdų iš kiekvienos klasės įrašysime į diską\n",
    "img_max = 10\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "print(\"\\nBendras aplankas vaizdams:\", images_folder)\n",
    "print(\"\\nMax sekų vaizdų skaičius iš kiekvienos klasės:\", img_max)\n",
    "\n",
    "if (img_max == 0):\n",
    "    sys.exit()\n",
    "\n",
    "# sukuriame bendrą aplanką vaizdams\n",
    "images_dir = Path(sets_path, 'saved_images', images_folder)\n",
    "create_dir(images_dir)\n",
    "\n",
    "# klasių simbolinių vardų sąrašas\n",
    "class_names = selected_beats.keys()\n",
    "\n",
    "# sukuriame aplankus sekų vaizdams klasėse\n",
    "create_subdir(images_dir, class_names)\n",
    "\n",
    "# Sukuriame vaizdų klasėse skaitiklį\n",
    "n_classes = len(class_names)\n",
    "skait = np.zeros((n_classes, n_classes), dtype=int)\n",
    "# print(skait)\n",
    "\n",
    "# sukuriame selected_beats reversiją\n",
    "rev_dict = get_rev_dictionary(selected_beats)\n",
    "\n",
    "y_test = df_seq_errors['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors['preds'].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "# *********************** derinimui *******************************\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nAccuracy: {acc:.2f}\\n\")\n",
    "# cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "# show_confusion_matrix(cnf_matrix, class_names)\n",
    "# ******************************************************************\n",
    "\n",
    "# Ciklas per sekas\n",
    "\n",
    "icycle = 0 \n",
    "leng = len(y_pred)\n",
    "\n",
    "for idx in validate_ind_lst:\n",
    "    if (icycle >= leng):\n",
    "        continue\n",
    "\n",
    "# *********************** derinimui ******************************   \n",
    "    # row = get_seq_attributes(all_beats_attr, idx)\n",
    "    # print(idx, row)\n",
    "# ****************************************************************\n",
    "\n",
    "    # anotuotos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    label = y_test[icycle]\n",
    "    label_symb = rev_dict[label]\n",
    "\n",
    "    # print(f\" {idx} label {label} symb {label_symb}\")\n",
    "\n",
    "    # klasifikatoriaus priskirtos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    pred = y_pred[icycle]\n",
    "    pred_symb = rev_dict[pred]\n",
    "    icycle +=1\n",
    "\n",
    "    # print(f\" {idx} pred {pred} symb {pred_symb}\")\n",
    "\n",
    "    # patikriname, ar neviršytas skaitiklis, jei viršytas, peršokame\n",
    "    if (skait[label,pred] >= img_max):\n",
    "        continue\n",
    "    else:\n",
    "        skait[label,pred] += 1\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "    seq_name = str(SubjCode) + '_' + str(idx)\n",
    "    \n",
    "     # 'Išpjauname' užduoto ilgio sekas ir sukuriame jų vaizdus\n",
    "    fig = plt.figure(facecolor=(1, 1, 1), figsize=(18,3))\n",
    "    # print(\"rec_dir =\", rec_dir)\n",
    "    ax = read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, wl_side, wr_side, wl_side_ext, wr_side_ext) \n",
    "    if (ax == None):\n",
    "        print(f'Sekai {idx} negali suformuoti išplėstinio vaizdo')\n",
    "        plt.close()\n",
    "        continue\n",
    "\n",
    "    # suformuosime koreguotą failo vardą\n",
    "    file_name = seq_name + '_' + pred_symb + \".png\" \n",
    "\n",
    "    # suformuosime kelią į atitinkamą sub-aplanką\n",
    "    image_subdir = Path(images_dir, label_symb)\n",
    "    if (os.path.exists(image_subdir) == False):\n",
    "        print('Klaida! ', image_subdir,' neegzistuoja')\n",
    "    file_path = Path(image_subdir, file_name)\n",
    "    # print('file_name: ',file_name, 'file_path: ', file_path)   \n",
    "\n",
    "    # Įrašome į atitinkamą anotacijai sub-aplanką \n",
    "    ax.set_title(file_name)\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches = 0.2)\n",
    "    plt.close()\n",
    "\n",
    "    # if (icycle >= len(y_pred)):\n",
    "        # break\n",
    "\n",
    "# ciklo per validate_ind_lst pabaiga\n",
    "\n",
    "print(\"\\n\")\n",
    "df = cm2df(skait, class_names)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nPabaiga.........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasifikavimo rodiklių skaičiavimas vienam nurodytam įrašui\n",
    "\n",
    "\n",
    "userNr, recordingNr = 1005, 1\n",
    "file_name = get_filename(rec_dir, create_SubjCode(userNr, recordingNr))\n",
    "\n",
    "# userId, recordingId =  get_rec_Id(rec_dir, userNr, file_name)\n",
    "print(\"\\nZive įrašas:\")\n",
    "print(f\"userNr: {userNr} recordingNr: {recordingNr} file_name: {file_name}\")\n",
    "# print(f\"\\nuserId: {userId}  recordingId: {recordingId}\")\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "y_test = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "print(\"\\nAnotacijų pasiskirstymas įraše:\")\n",
    "cols, dist, tot = get_freq_unique_values(y_test, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot}\\n \")\n",
    "\n",
    "noise_arr = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['noises'].to_numpy(dtype=int)\n",
    "Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "Err = get_error(y_test, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f} Error(%): {Err}\\n \")\n",
    "cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "show_confusion_matrix(cnf_matrix, class_names)\n",
    "# https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "# *********************************************************************************************************\n",
    "\n",
    "prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "row = {\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "\n",
    "print(\"\\n\")\n",
    "tit1 = f\"{'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "str1 =f\"{int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "print(str1+str2+str3+str4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
