{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS in my system :  win32\n",
      "\n",
      "Skriptas zive įrašų transformacijai\n",
      "\n",
      "Išeities duomenys skaitomi iš: D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_selected\n",
      "Transformuoti duomenys rašomi į: D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šitame variante keičiamas list.json ir df_transl failai\n",
    "#\n",
    "# iš originalių Zive įrašų sukuriami įrašai *.npy, anotacijų failai *.json - nekeičiami,\n",
    "# koreguojamas list.json failas, pakeičiant file_name į SubjCode.\n",
    "\n",
    "# Planas:\n",
    "# sukuriame tuščią dataframe df_list_selected, iš kurio darysime naują list.json\n",
    "# Nuskaitome iš bendros talpyklos list.json\n",
    "# pasidarom sąrašą userId:recordId1, recordId2,.... \n",
    "# ciklas per visus pacientus userId:\n",
    "#   suformuojame sąrašą recordId: ekstrasistolių skaičius ir surušiuojame pagal ekstrasistolių\n",
    "#   skaičių mažėjančia tvarka\n",
    "#   Surikiavus įrašus, įrašus atrenkame tokiu būdu:\n",
    "# \t\ta. Atrenkame pirmiausiai tuos įrašus, kurie turi daugiausiai ekstrasistolių. \n",
    "#       Jei visi įrašai turi ekstrasistolių, tai atrenkame pirmus 10 (iš surikiuotų įrašų pagal ekstrasistolių kiekį).\n",
    "#       Jei ne visi įrašai turi ekstrasistoles, tai atrenkame, tik tuos įrašus, kurie turi ekstrasistoles\n",
    "#       (bet ne daugiau 10 įrašų), ir jei įrašų atrinkta ne daugiau 9, pridedame dar vieną įrašą be ekstrasistolių.\n",
    "# \t\tb. Jei pacientas ekstrasistolių neturi, tai atrenkame ne daugiau vieno (pirmo) to paciento įrašo\n",
    "\n",
    "#   Atrinkus įrašus, papildome df_list_selected  \n",
    "# ciklo per pacientus pabaiga\n",
    "# įrašome visus atrinkus įrašus į record_selected\n",
    "# įrašome df_list_selected į list.json.\n",
    "# išvedame į ekraną suvestinę ir df_list_selected\n",
    "\n",
    "# SubjCode = userNr + recordingNr -- reikia taisyti /////////////////////////////////////////////////////\n",
    "\n",
    "import shutil, sys\n",
    "from icecream import ic\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from zive_util_vu import zive_read_file_1ch, create_dir\n",
    "\n",
    "def get_recNr(rec_dir, userId, file_name):\n",
    "    # Skriptas Zive ekg įrašo identikatoriams userId, file_name pakeisti į virtualią numeraciją:\n",
    "    # userId pakeičiamas į userNr - skaičiumi, pradedant nuo 1000, įrašo įdentifikatorius file_name pakeičiams\n",
    "    # į įrašo eilės numerį registrationNr, lygų 0,1,...\n",
    "    # Konversijai ir saugojimui panaudojamas df masyvas df_transl\n",
    "    # Jei paciento Nr nėra - užvedamas įrašas\n",
    "\n",
    "    # Perdarytas iš varianto, kuriame buvo naudojamas recordingID. Pakeistas į file_name\n",
    "    \n",
    "    # Patikriname, ar df_transl egzistuoja. Jei ne, sukuriame ir įrašome pirmą įraša\n",
    "    file_path = Path(rec_dir, 'df_transl.csv')\n",
    "    if (not file_path.exists()):\n",
    "        # Paruošiame masyvą - žodyną numerių vertimui iš userId, registrationId į userNr, registrationNr ir atgal\n",
    "        # ir įrašome į diską\n",
    "        first_rec = {'userId':[userId], 'file_name':[file_name], 'userNr':[1000], 'recordingNr':[0]}\n",
    "        df_transl = pd.DataFrame(first_rec)\n",
    "        file_path = Path(rec_dir, 'df_transl.csv')\n",
    "        df_transl.to_csv(file_path)\n",
    "        # print(df_transl)\n",
    "        return df_transl.loc[0, 'userNr'], df_transl.loc[0, 'recordingNr']\n",
    "\n",
    "    # Jei egzistuoja, nuskaitome vardų žodyną iš rec_dir aplanko\n",
    "    file_path = Path(rec_dir, 'df_transl.csv')\n",
    "    df_transl = pd.read_csv(file_path, index_col=0)\n",
    "    # print(df_transl)\n",
    "    # Ieškome, ar yra įrašas su userId\n",
    "    # Jei userId nerandame, sukuriame naują įrašą su userId, recordingId, userNr, recordingNr \n",
    "    if (df_transl.loc[(df_transl['userId'] == userId)]).empty:\n",
    "        userNr = df_transl.loc[len(df_transl)-1, 'userNr'] + 1\n",
    "        # print(f\"{userNr=}\")\n",
    "        new_row = {'userId':userId, 'file_name':file_name, 'userNr':userNr, 'recordingNr':0}\n",
    "        df_transl = df_transl.append(new_row, ignore_index=True)\n",
    "        file_path = Path(rec_dir, 'df_transl.csv')\n",
    "        df_transl.to_csv(file_path)\n",
    "        return userNr, 0\n",
    "    \n",
    "    # Jei radus userId, randame kad šio paciento įrašo file_name jau yra, su įrašais nieko nedarome\n",
    "    row = df_transl.loc[(df_transl['userId'] == userId) & (df_transl['file_name'] == file_name)]\n",
    "    if not row.empty:\n",
    "        # print(row)\n",
    "        return row['userNr'].values[0], row['recordingNr'].values[0]\n",
    "    else:\n",
    "    # Jei šio paciento įrašo su file_name nėra, formuojame įrašą\n",
    "        rows = df_transl.loc[(df_transl['userId'] == userId)]\n",
    "        recordingNr = max(rows['recordingNr'].to_list()) + 1\n",
    "        userNr = rows['userNr'].values[0]\n",
    "        new_row = {'userId':userId, 'file_name':file_name, 'userNr':userNr, 'recordingNr':recordingNr}\n",
    "        df_transl = df_transl.append(new_row, ignore_index=True)\n",
    "        file_path = Path(rec_dir, 'df_transl.csv')\n",
    "        df_transl.to_csv(file_path)\n",
    "        return userNr, recordingNr    \n",
    "    \n",
    "def create_SubjCode(userNr, recordingNr):\n",
    "    # SubjCode = userNr + recordingNr\n",
    "    # pvz. SubjCode = 10002\n",
    "    if (userNr < 1000):\n",
    "        return userNr\n",
    "    else:        \n",
    "        str_code = str(userNr) + str(recordingNr)\n",
    "        SubjCode = int(str_code)\n",
    "        return SubjCode\n",
    "\n",
    "# def get_userNr(rec_dir, userId):  # Su išmestu recordingId\n",
    "#     # Panaudodami df masyvą df_transl su įrašų numeriais iš ZIVE numerių gauname įrašų eilės numerius\n",
    "#     # Jei paciento Nr nėra - užvedamas įrašas\n",
    "    \n",
    "#     # Patikriname, ar df_transl egzistuoja. Jei ne, sukuriame ir įrašome pirmą įraša\n",
    "#     file_path = Path(rec_dir, 'df_transl.csv')\n",
    "#     if (not file_path.exists()):\n",
    "#         # Paruošiame masyvą - žodyną numerių vertimui iš userId, registrationId į userNr, registrationNr ir atgal\n",
    "#         # ir įrašome į diską\n",
    "#         first_rec = {'userId':[userId], 'userNr':[1000] }\n",
    "#         df_transl = pd.DataFrame(first_rec)\n",
    "#         file_path = Path(rec_dir, 'df_transl.csv')\n",
    "#         df_transl.to_csv(file_path)\n",
    "#         return df_transl.loc[0, 'userNr']\n",
    "\n",
    "#     # Jei egzistuoja, nuskaitome vardų žodyną iš rec_dir aplanko\n",
    "#     file_path = Path(rec_dir, 'df_transl.csv')\n",
    "#     df_transl = pd.read_csv(file_path, index_col=0)\n",
    "#     # print(df_transl)\n",
    "#     # Ieškome, ar yra įrašas su userId\n",
    "#     # Jei userId nerandame, sukuriame naują įrašą su userId, userNr \n",
    "#     if (df_transl.loc[(df_transl['userId'] == userId)]).empty:\n",
    "#         userNr = df_transl.loc[len(df_transl)-1, 'userNr'] + 1\n",
    "#         print(\"userNr=\", userNr)\n",
    "#         new_row = {'userId':userId, 'userNr':userNr}\n",
    "#         df_transl = df_transl.append(new_row, ignore_index=True)\n",
    "#         file_path = Path(rec_dir, 'df_transl.csv')\n",
    "#         df_transl.to_csv(file_path)\n",
    "#         return userNr\n",
    "#     else:\n",
    "#         # Jei userId randame, gražiname userNr\n",
    "#         row = df_transl.loc[(df_transl['userId'] == userId)]\n",
    "#         return row['userNr'].values[0]\n",
    "\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Nuoroda į aplanką su EKG duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "rec_dir = Path(db_path,'records_selected')\n",
    "rec_list = 'list_tst.json'\n",
    "\n",
    "# Nuoroda į aplanką su transformuotu EKG duomenų rinkiniu\n",
    "rec_dir_npy = Path(db_path, 'records_npy')\n",
    "\n",
    "# Paliekamų anotacijų sąrašas\n",
    "annot_list = ['N','S','V','U']\n",
    "\n",
    "# Diskretizavimo dažnis\n",
    "fs_zive = 200\n",
    "\n",
    "print(\"\\nSkriptas zive įrašų transformacijai\\n\")\n",
    "\n",
    "print(\"Išeities duomenys skaitomi iš:\", rec_dir)\n",
    "print(\"Transformuoti duomenys rašomi į:\", rec_dir_npy)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy' already exists\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000027BE9302AC8>\n",
      "\n",
      "\n",
      "[10003, 10011, 10021, 10033, 10034, 10035, 10041, 10054, 10055, 10056, 10057, 10061, 10071, 10081, 10092, 10093, 10101, 10112, 10113, 10121, 10131]\n",
      "\n",
      "Failų sąrašas įrašytas:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\\list_tst.json\n",
      "\n",
      "Schemos parametrai įrašyti į failą:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\\info_create_z.json \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sukūriame aplanką EKG sekų įrašymui\n",
    "create_dir(rec_dir_npy)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000, \"display.max_columns\", 20)\n",
    "pd.set_option('display.width', 2000)\n",
    "\n",
    "# Nuskaitome Zive įrašų talpykloje laikomų įrašų sąrašą\n",
    "file_path = Path(rec_dir, rec_list)\n",
    "with open(file_path,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "df_list = pd.json_normalize(data, record_path =['data'])\n",
    "\n",
    "# print(df_list)\n",
    "# Sugrupuojame įrašus į to paties paciento grupes\n",
    "grouped = df_list.groupby(['userId','file_name'])\n",
    "print(grouped)\n",
    "print(\"\\n\")\n",
    "\n",
    "SubjCodes = []\n",
    "\n",
    "# Perrašome įrašus nauju formatu į kitą folderį\n",
    "for key in grouped.groups:\n",
    "    userId = key[0]\n",
    "    file_name = str(key[1])\n",
    "    \n",
    "    userNr, recordingNr = get_recNr(rec_dir_npy, userId, file_name)\n",
    "    SubjCode = create_SubjCode(userNr, recordingNr)  \n",
    "    # print(\"SubjCode: \", SubjCode, userNr, file_name)\n",
    "\n",
    "    SubjCodes.append(SubjCode)\n",
    "\n",
    "    file_path = Path(rec_dir, file_name)\n",
    "    signal = zive_read_file_1ch(file_path)  \n",
    "\n",
    "    file_path = Path(rec_dir_npy, str(SubjCode) + '.npy')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        np.save(f, signal)\n",
    "\n",
    "    src = Path(rec_dir, file_name + '.json')\n",
    "    dst = Path(rec_dir_npy, str(SubjCode) + '.json')\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "print(SubjCodes)\n",
    "\n",
    "# Papildome įrašų sarašą df_list stulpeliu SubjCode\n",
    "df_list.insert(0, 'SubjCode', SubjCodes, True)\n",
    "\n",
    "# Įrašome failų sąrašą į diską\n",
    "file_path = Path(rec_dir_npy,'list_tst.json')       \n",
    "df_list.to_json(file_path, orient = 'table', index=False)\n",
    "print(f'\\nFailų sąrašas įrašytas:  {file_path}')\n",
    "\n",
    "\n",
    "info = {\n",
    "    'db_folder':db_folder,\n",
    "    'fs': fs_zive,\n",
    "    'SubjCodes':SubjCodes,\n",
    "    'annot_list':annot_list\n",
    "    }\n",
    "\n",
    "file_name = Path(rec_dir_npy,'info_create_z.json')\n",
    "with open(file_name, 'w') as f:\n",
    "    json.dump(info, f)\n",
    "    \n",
    "print(\"\\nSchemos parametrai įrašyti į failą: \", file_name, \"\\n\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Viso pacientų: 14  EKG įrašų: 21\n",
      "\n",
      "Įrašų pasiskirstymas per pacientus\n",
      "                      userID     N    S    V  U  count\n",
      "0   61632b451e325518bb017bbd   720    1    0  0      1\n",
      "1   6190d4e63cd1d227c1303cea  1154    3    1  0      2\n",
      "2   6144c588bd0cc52ba0275362  1558    0  122  0      3\n",
      "3   61a8863e5cd547e980a4e970  1277  189   42  0      2\n",
      "4   61632ace1e32557a62017bba  3920  321    3  0      4\n",
      "5   6144c532bd0cc54c83275360   787    0    0  0      1\n",
      "6   6143507abd0cc5051b275171   756    1    4  0      1\n",
      "7   61b32023cf0f347968cb1c91   698    1    0  0      1\n",
      "8   6144c682bd0cc5acb7275368   716    0   96  0      1\n",
      "9   61b31ff1cf0f34275ccb1c90   637    2   74  0      1\n",
      "10  619b4bbbb481320f924e3da9   645    1    0  0      1\n",
      "11  616b00a811601579e9c2816c   821    0    0  0      1\n",
      "12  6144c4fbbd0cc552e427535f   540    0  205  0      1\n",
      "13  617509686d4fb48519637743   796   12    1  0      1\n",
      "\n",
      "       sum:  15025, 531, 548, 0, 21\n",
      "      total:  16125\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-shutil-copy2-method/#:~:text=shutil.copy\n",
    "# 2%20%28%29%20method%20in%20Python%20is%20used%20to,destination%20can%20be%20a%20file%20or%20a%20directory.\n",
    "\n",
    "# Susirandame anotacijų pasiskirstymą per pacientus ir pacientų skaičių\n",
    "df_sum = df_list.groupby(['userId'],sort = False).sum()\n",
    "# print(df_sum)\n",
    "# https://sparkbyexamples.com/pandas/pandas-groupby-sum-examples/\n",
    "count = df_list['userId'].value_counts()\n",
    "print(f'\\nViso pacientų: {len(count)}  EKG įrašų: {len(df_list)}')\n",
    "print(f'\\nĮrašų pasiskirstymas per pacientus')\n",
    "count = count.rename(\"count\")\n",
    "frames = [df_sum, count]\n",
    "result = pd.concat(frames, axis = 1)\n",
    "result.index.rename ('userID', inplace= True)\n",
    "result = result.reset_index()\n",
    "\n",
    "result.drop(labels=['incl'], axis=1, inplace=True)\n",
    "result.drop(labels=['flag'], axis=1, inplace=True)\n",
    "result.drop(labels=['SubjCode'], axis=1, inplace=True)\n",
    "# https://www.shanelynn.ie/pandas-drop-delete-dataframe-rows-columns/\n",
    "print(result)\n",
    "\n",
    "suma = result.iloc[:,1:].sum()\n",
    "print('\\n',' '*5, 'sum: ',str(suma.tolist())[1:-1])\n",
    "total = suma.sum()   # pataisyti, kad sumuotų tik pūpsnius\n",
    "print(' '*5, 'total: ', total)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fdd05f7b6e7f46fd1f1bbcbfdc9d8b4b1f98b078b306375c0cb77e6ad3f81a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ecg': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
